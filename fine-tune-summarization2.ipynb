{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl evaluate matplotlib tensorboard rouge_score bert_score\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "from utils import prepare_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEE A DEV:\n",
    "-lire papiers de recherches\n",
    "-Tester modele encodeur decodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = load_dataset('json', data_files='dataset_llm_generated.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_raw.select_columns([\"text\", \"summary\"])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distibution')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_word(example):\n",
    "    return {\"num_word\": len(example[\"summary\"].split())}\n",
    "\n",
    "dataset_num_word = dataset['train'].map(get_num_word)\n",
    "\n",
    "plt.hist(dataset_num_word[\"num_word\"], bins=100)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Total\")\n",
    "plt.title(\"Distibution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/tiktoken/load.py:154\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     token, rank \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    155\u001b[0m     ret[base64\u001b[38;5;241m.\u001b[39mb64decode(token)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rank)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1722\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1622\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1622\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1624\u001b[0m         [\n\u001b[1;32m   1625\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1626\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1627\u001b[0m         ]\n\u001b[1;32m   1628\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1615\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1615\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1616\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1591\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[0;32m-> 1591\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1592\u001b[0m byte_encoder \u001b[38;5;241m=\u001b[39m bytes_to_unicode()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/tiktoken/load.py:157\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken_bpe_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mValueError\u001b[0m: Error parsing line b'\\x0e' in /Data/gabriel-mercier/slm_models/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/spiece.model",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m slm_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/mt5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenizer_slm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Data/gabriel-mercier/slm_models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer_slm\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer_slm\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:944\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    942\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         )\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2292\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2292\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2294\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2297\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:119\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_ids \u001b[38;5;241m=\u001b[39m extra_ids\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 139\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1727\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1723\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1724\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1725\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1729\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1731\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "slm_name = \"google/mt5-base\"\n",
    "#slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer_slm = AutoTokenizer.from_pretrained(slm_name, cache_dir=\"/Data/gabriel-mercier/slm_models\")\n",
    "tokenizer_slm.pad_token = tokenizer_slm.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "summary_num_tokens = 200\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer_slm(examples[\"text\"], truncation=True, max_length=max_length)\n",
    "    targets = tokenizer_slm(examples[\"summary\"], truncation=True, max_length=summary_num_tokens)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_train_temp = tokenized_datasets[\"train\"].train_test_split(test_size=0.4, seed=42)\n",
    "split_valid_test = split_train_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": split_train_temp[\"train\"],        \n",
    "    \"validation\": split_valid_test[\"train\"],      \n",
    "    \"test\": split_valid_test[\"test\"]              \n",
    "})\n",
    "\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 896)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#lora_alpha = 2 * rank\n",
    "attn_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "mlp_modules = [\"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(r=16, \n",
    "                         lora_alpha=32,\n",
    "                         target_modules=attn_modules,\n",
    "                         lora_dropout=0.05,\n",
    "                         bias='none',\n",
    "                         task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model_raw, lora_config)\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2162688 || all params: 317282176 || trainable%: 0.6816292132338376\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsume prcisment le texte suivant en franais en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. vite les phrases inutiles et reformule les ides clairement.\n",
      "\n",
      "Texte :\n",
      "Troy Davis est prsent par de nombreuses personnalits comme le prototype du Noir innocent, condamn  mort pour le meurtre d'un policier blanc en 1989. AP/Thibault Camus Plus de trois cent rassemblements ont t organiss vendredi 16 septembre aux Etats-Unis et dans le monde pour rclamer la clmence pour Troy Davis, un condamn  mort qui doit tre excut mercredi, dont le sort est dsormais entre les mains du comit des grces de Gorgie. L'excution de Troy Davis, symbole international de la lutte contre la peine de mort, a t programme par injection ltale mercredi  la prison de Jackson, malgr les doutes sur sa culpabilit. Le comit des grces de Gorgie est appel lundi  commuer ou non la peine capitale de Davis en prison  vie. Une ptition runissant plus de 663 000 signatures rclamant la grce de Davis lui a t remise jeudi, selon ses soutiens. \"Il y a trop de doutes dans cette affaire, nous esprons qu'ils entendront le message: est-on sr que nous n'allons pas excuter un innocent ?\", a dclar Laura Moye, directrice de la campagne pour l'abolition de la peine de mort  Amnesty international. A Paris, environ 150 manifestants, dont une grande majorit vtue de tee-shirts arborant le portrait de Troy Davis, se sont runis  l'appel de plusieurs organisations. Aux Etats-Unis, treize rassemblements taient organiss en fin d'aprs-midi aux quatre coins du pays. A Chicago, devant le sige de campagne de Barack Obama, environ 200 personnes se sont rassembles, scandant \"Librez Troy Davis !\". Dans la capitale fdrale Washington, une centaine de personnes ont manifest habills de tee-shirts bleus \"Justice pour Troy Davis\" et portant des pancartes \"Fin de l'Etat meurtrier\", \"La peine de mort doit cesser\", ou \"Le couloir de la mort de Georgie est raciste\". Troy Davis a t excut le 21 septembre 2011, par injection ltale au pnitencier de Jackson, en Gorgie. AFP/HO LA FRANCE EXPRIME SA \"PROCCUPATION\" Ag de 42 ans, dont 20 passs dans le couloir de la mort en Gorgie, Troy Davis est prsent par de nombreuses personnalits comme le prototype du Noir innocent, condamn  mort pour le meurtre d'un policier blanc en 1989. Il a t soutenu par des personnalits comme Jimmy Carter, le Pape Benot XVI ou l'actrice Susan Sarandon. La France pour sa part a exprim vendredi sa \"proccupation\". Neuf tmoins ont dsign  l'poque Troy Davis comme l'auteur du coup de feu mais l'arme du crime n'a jamais t retrouve et aucune empreinte digitale ou ADN n'a t releve. Depuis, sept tmoins sont revenus sur leurs dclarations, dont certains ont dsign un autre tireur. La Cour suprme avait offert  Troy Davis la possibilit exceptionnelle, en aot 2009, de bnficier d'une nouvelle audience. Plusieurs tmoins avaient racont, sans convaincre le juge fdral, comment la police les avait persuads  l'poque de dsigner le jeune Noir.\n",
      "\n",
      "Rsum concis et structur (100 mots maximum) :\n"
     ]
    }
   ],
   "source": [
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_start = \"Rsum concis et structur (100 mots maximum) :\"\n",
    "\n",
    "def example1():\n",
    "    \n",
    "\n",
    "    encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "    start_index = prediction.find(assistant_start)\n",
    "    if start_index != -1:\n",
    "        response_start = start_index + len(assistant_start)\n",
    "    else:\n",
    "        response_start = 0 \n",
    "\n",
    "    print(\"=== GENERATED SUMMARY ===\")\n",
    "    print(prediction[response_start:])\n",
    "    print(len(prediction[response_start:].split()))\n",
    "    print(\"=== LABEL SUMMARY ===\")\n",
    "    print(summary_data)\n",
    "    print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prepare_prompt(data_point)+tokenizer_slm.eos_token \n",
    "    #print(f\"full_prompt {full_prompt}\")\n",
    "    tokenized_full_prompt = tokenizer_slm(full_prompt, return_tensors='pt')\n",
    "    labels = tokenized_full_prompt.input_ids.clone() ## FILL THE GAP: create the labels first by cloning input_ids\n",
    "    \n",
    "    # prompt = full_prompt[:full_prompt.find(\"Rsum\")] + \"Rsum\"\n",
    "    \n",
    "    assistant_token = tokenizer_slm(\"Rsum concis et structur\", return_tensors='pt')['input_ids'][0]\n",
    "    T = tokenized_full_prompt['input_ids'].flatten()\n",
    "    S = assistant_token.flatten()\n",
    "    \n",
    "    for i in range(len(T) - len(S) + 1):\n",
    "        if torch.equal(T[i:i+len(S)], S):\n",
    "            end_prompt_idx = i+len(S)   ## FILL THE GAP: get the index of the '<assistant>:' (or the equivalent token) in order to replace all but response tokens with -100\n",
    "    labels[:, :end_prompt_idx] = -100\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_split[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_val = dataset_split[\"validation\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_test = DatasetDict({\n",
    "    \"test\": dataset_split[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"L'amnistie aura lieu avant la visite du prsident [Thein Sein] en Inde\" mercredi, a assur un responsable briman, lundi. AP/Khin Maung Win La tlvision d'Etat en Birmanie annonce, mardi 11 octobre, la libration de plus de 6 300 \"prisonniers\", sans prciser si des dtenus politiques comptaient parmi les bnficiaires mais alors que les annonces en ce sens se sont multiplies dans le pays ces derniers jours. Les premires librations interviendront mercredi. La commission nationale pour les droits de l'homme, mise en place en septembre par le gouvernement, avait rclam, quelques heures auparavant, dans un quotidien officiel la libration des \"prisonniers de conscience\" afin de rpondre aux appels en ce sens de la communaut internationale. Dans une lettre ouverte publie par le quotidien anglophone New Light of Myanmar, considr comme le porte-parole du rgime, la commission rappelle mardi que \"le secrtaire gnral des Nations unies et un certain nombre de pays rclament la libration de ce qu'ils dcrivent comme des 'prisonniers de conscience'\". La commission \"demande humblement au prsident, en gage de sa magnanimit, d'amnistier ces prisonniers et de les librer de prison\", a conclu son prsident, Win Mya, dans une rare reconnaissance de l'existence de prisonniers politiques dans le pays. Lundi, des responsables gouvernementaux avaient dj indiqu qu'une amnistie incluant des prisonniers politiques aurait lieu dans les jours suivants. \"L'amnistie aura lieu avant la visite du prsident [Thein Sein] en Inde\" mercredi, a assur un responsable. Les Etats-Unis, l'Union europenne et l'opposition dmocratique birmane rclament la libration de quelque deux mille prisonniers politiques  militants politiques, avocats, artistes, journalistes  pour prouver la sincrit des rformes politiques actuelles. La mesure est aussi considre comme une condition sine qua non pour envisager la leve des sanctions conomiques et politiques en place depuis la fin des annes 1990.\n",
      "La tlvision d'tat birmane annonce la libration prochaine de plus de 6 300 prisonniers. Les premires librations interviendront mercredi, avant la visite du prsident Thein Sein en Inde. La commission nationale des droits de l'homme demande l'amnistie des \"prisonniers de conscience\". Des responsables gouvernementaux ont confirm une amnistie incluant des prisonniers politiques. Les tats-Unis, l'UE et l'opposition birmane rclament la libration de quelque 2 000 prisonniers politiques pour prouver la sincrit des rformes. Cette mesure est vue comme une condition pour lever les sanctions conomiques et politiques en place depuis 1990. Le texte met pour la premire fois en lumire l'existence de prisonniers politiques en Birmanie. Rsum concis et structur (100 mots maximum) :\n",
      "[84836, 31323, 132971, 41525, 512, 67967, 45832, 517, 662, 54367, 662, 220, 16, 15, 15, 77099, 7192, 13, 61161, 265, 4686, 72, 1729, 3541, 3501, 3956, 22396, 2010, 15510, 25437, 2676, 294, 6, 453, 83896, 12788, 409, 3980, 17276, 13, 28024, 85, 632, 3541, 31747, 304, 332, 3658, 1842, 14836, 1111, 3541, 877, 13700, 1185, 11998, 478, 382, 1178, 68, 6260, 73630, 57491, 64759, 645, 39042, 38281, 32570, 1187, 74736, 3845, 88702, 508, 785, 258, 1345, 258, 60, 662, 2263, 68, 1, 4704, 837, 8579, 11, 264, 1071, 324, 963, 650, 76832, 1411, 39210, 11, 326, 55898, 13, 10106, 32854, 41557, 11331, 2185, 12190, 4929, 42016, 13013, 294, 6, 31860, 266, 662, 36819, 1515, 645, 45220, 11, 296, 36389, 220, 16, 16, 18491, 37608, 11, 1187, 3051, 52201, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 330, 649, 3335, 77, 4813, 497, 15510, 50525, 12059, 4403, 939, 7439, 1960, 355, 3354, 8303, 469, 51591, 1167, 1346, 8155, 3541, 83133, 69, 24024, 3861, 9870, 44475, 1709, 3541, 75739, 662, 3761, 6097, 511, 14789, 7299, 500, 72, 13700, 6866, 512, 21241, 26652, 35752, 4813, 48201, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 13, 4929, 12123, 6995, 1574, 4914, 3541, 96122, 409, 326, 6, 86613, 11, 56359, 662, 1992, 662, 94643, 1346, 512, 84082, 39180, 11, 49490, 9333, 564, 309, 963, 11, 44789, 75045, 97444, 277, 402, 517, 11, 6866, 650, 98468, 3591, 2786, 13029, 1187, 3051, 52201, 939, 330, 649, 3335, 77, 4813, 409, 41463, 1, 53301, 409, 74771, 265, 10047, 906, 2010, 662, 3761, 6097, 409, 1187, 141032, 2590, 37035, 13, 45606, 6185, 69456, 5908, 64832, 77411, 7888, 1346, 512, 98468, 3591, 6454, 385, 4844, 1532, 8658, 315, 52355, 11, 76454, 67762, 21572, 512, 59072, 56998, 1263, 3845, 137269, 11, 1187, 12123, 60058, 6712, 296, 36389, 1709, 330, 273, 511, 129532, 70137, 88075, 939, 19140, 650, 550, 1842, 650, 3654, 12736, 409, 21241, 9333, 564, 2838, 1187, 3051, 52201, 409, 3761, 922, 84117, 7439, 5082, 80441, 21572, 939, 364, 649, 3335, 77, 4813, 409, 41463, 6, 3263, 4929, 12123, 330, 82931, 68, 38512, 478, 7906, 88702, 11, 662, 342, 424, 409, 822, 8455, 276, 98381, 11, 294, 57491, 64759, 1268, 26652, 9343, 77, 4813, 1842, 409, 3541, 3051, 51324, 409, 9343, 497, 264, 92124, 84, 4438, 88702, 11, 12190, 3017, 64, 11, 6866, 6185, 8848, 87505, 409, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 6866, 512, 21241, 13, 444, 55898, 11, 939, 4200, 4788, 84082, 39180, 11981, 83664, 1167, 45839, 1257, 75266, 922, 30009, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 43421, 1315, 38281, 6866, 3541, 48201, 45832, 1783, 13, 330, 43, 57491, 64759, 645, 39042, 38281, 32570, 1187, 74736, 3845, 88702, 508, 785, 258, 1345, 258, 60, 662, 2263, 68, 1, 4704, 837, 8579, 11, 264, 1071, 324, 963, 650, 76832, 13, 11615, 18888, 1862, 82245, 285, 11, 326, 6, 32658, 140927, 1842, 326, 6, 453, 3487, 78541, 72532, 2372, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 25552, 296, 4517, 9343, 77, 4813, 3354, 8303, 1365, 38349, 3354, 8303, 11, 1822, 509, 1862, 11, 10049, 288, 11, 22825, 288, 1365, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 3354, 8303, 1160, 64732, 13, 4929, 83821, 1788, 27363, 76454, 13763, 7888, 21572, 6185, 2971, 57668, 73525, 2477, 4914, 84675, 1409, 1187, 22638, 7888, 939, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 1187, 1875, 939, 64738, 220, 16, 24, 24, 15, 382, 84836, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 14512, 8747, 42016, 13013, 294, 6, 135212, 15248, 1515, 68, 45220, 1187, 3051, 52201, 462, 331, 8346, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 9343, 77, 4813, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 11, 32570, 1187, 74736, 3845, 88702, 576, 258, 1345, 258, 662, 2263, 68, 13, 4929, 12123, 6995, 1574, 939, 96122, 409, 326, 6, 86613, 61063, 326, 57491, 64759, 645, 939, 330, 649, 3335, 77, 4813, 409, 41463, 3263, 3874, 4200, 4788, 84082, 39180, 11981, 14508, 7683, 963, 6185, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 13, 11615, 28024, 83, 1862, 82245, 285, 11, 326, 6, 2230, 1842, 326, 6, 453, 3487, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 220, 17, 220, 15, 15, 15, 9343, 77, 4813, 3354, 8303, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 13, 61308, 83821, 1788, 47134, 21572, 6185, 2971, 4914, 27505, 3541, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 220, 16, 24, 24, 15, 13, 1967, 67967, 2270, 4914, 1187, 54033, 36191, 662, 141096, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 662, 36819, 1515, 645, 13, 50123, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 549, 151645]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 320, 16, 15, 15, 77099, 7192, 8, 14512, 8747, 42016, 13013, 294, 6, 135212, 15248, 1515, 68, 45220, 1187, 3051, 52201, 462, 331, 8346, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 9343, 77, 4813, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 11, 32570, 1187, 74736, 3845, 88702, 576, 258, 1345, 258, 662, 2263, 68, 13, 4929, 12123, 6995, 1574, 939, 96122, 409, 326, 6, 86613, 61063, 326, 57491, 64759, 645, 939, 330, 649, 3335, 77, 4813, 409, 41463, 3263, 3874, 4200, 4788, 84082, 39180, 11981, 14508, 7683, 963, 6185, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 13, 11615, 28024, 83, 1862, 82245, 285, 11, 326, 6, 2230, 1842, 326, 6, 453, 3487, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 220, 17, 220, 15, 15, 15, 9343, 77, 4813, 3354, 8303, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 13, 61308, 83821, 1788, 47134, 21572, 6185, 2971, 4914, 27505, 3541, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 220, 16, 24, 24, 15, 13, 1967, 67967, 2270, 4914, 1187, 54033, 36191, 662, 141096, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 662, 36819, 1515, 645, 13, 50123, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 549, 151645]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[1]['text'])\n",
    "print(dataset_train[1]['summary'])\n",
    "print(dataset_train[1]['input_ids'])\n",
    "print(dataset_train[1]['labels'])\n",
    "print(dataset_train[1]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46484375 Memory used before training (GB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved()/1024**3, 'Memory used before training (GB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46484375 Memory used before training (GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/lib64/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.544400</td>\n",
       "      <td>1.449584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.384800</td>\n",
       "      <td>1.416431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>1.413173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.671875 Memory used after training (GB)\n",
      "14.671875 Memory used before training (GB)\n",
      "train length 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.536200</td>\n",
       "      <td>1.362942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.347800</td>\n",
       "      <td>1.360146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.219800</td>\n",
       "      <td>1.359396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.537109375 Memory used after training (GB)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import transformers\n",
    "\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=2,\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=45,   \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "\n",
    "def train_on_subset(fraction):\n",
    "    global model\n",
    "    #shuffle and sample fraction\n",
    "    subset_train = dataset_train.shuffle().select(range(int(len(dataset_train)*fraction)))\n",
    "    subset_val = dataset_val.shuffle().select(range(int(len(dataset_val)*fraction)))\n",
    "    print('train length', len(subset_train))\n",
    "\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=subset_train,\n",
    "        eval_dataset=subset_val,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer_slm, model=model),\n",
    "        )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "repeat = 2\n",
    "for _ in range(repeat):    \n",
    "    print(torch.cuda.memory_reserved()/1024**3 , 'Memory used before training (GB)') \n",
    "    train_on_subset(0.02)\n",
    "    #torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "    print(torch.cuda.memory_reserved()/1024**3 , 'Memory used after training (GB)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.537109375  (GB) Memory used during training\n",
      "0.529296875  (GB) Memory used during training\n",
      "Model Parameters Memory Usage: 438.71826171875 MB\n",
      "Total gradient memory usage: 0 MB\n",
      "Dataset size: 3.4196739196777344 MB\n",
      "5.742784\n",
      "5.743312\n",
      "41.115352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.memory_reserved()/1024**3, ' (GB) Memory used during training')\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved()/1024**3, ' (GB) Memory used during training')\n",
    "print(f\"Model Parameters Memory Usage: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 ** 2} MB\")\n",
    "# Track gradient memory usage during the backward pass\n",
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_size = param.grad.element_size() * param.grad.numel() / 1024 ** 2  # in MB\n",
    "        total += grad_size\n",
    "        #print(f\"{name} - Gradient memory usage: {grad_size} MB\")\n",
    "\n",
    "print(f\"Total gradient memory usage: {total} MB\")\n",
    "print(f\"Dataset size: {sum(len(item['input_ids']) for item in dataset_train) / 1024 ** 2} MB\")\n",
    "\n",
    "from pympler import asizeof\n",
    "\n",
    "# Get the full memory usage of the variable (including all referenced objects)\n",
    "\n",
    "print(asizeof.asizeof(model)/10**6)  # In bytes\n",
    "print(asizeof.asizeof(model.named_parameters())/10**6)  # In bytes\n",
    "print(asizeof.asizeof(dataset)/10**6)  # In bytes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SUMMARY ===\n",
      "Rsum concis et structur (100 mots maximum) : Troy Davis, condamn  mort pour le meurtre d'un policier blanc en 1989, a t prsent par plusieurs personnalits comme le prototype du Noir innocent. Le comit des grces de Gorgie a appel lundi  commuer ou non la peine capitale de Davis en prison  vie. Des rassemblements ont eu lieu en Californie, en Amrique du Nord et en Europe. Troy Davis a t excut le 21 septembre 2011, aprs avoir t condamn par l'arme amricaine. Les forces de l'ordre ont recueilli des tmoins confirmant son arrestation. Le juge fdral a donn son accord pour une nouvelle audition, mais Davis ne s'est pas rendu  l'pisode. La France a exprim sa proccupation face  des doutes sur sa culpabilit. Les ras\n",
      "130\n",
      "=== LABEL SUMMARY ===\n",
      "Troy Davis, condamn  mort pour le meurtre d'un policier en 1989, doit tre excut malgr des doutes sur sa culpabilit. Des manifestations mondiales rclament sa clmence. Le sort de Davis, 42 ans, est entre les mains du comit des grces de Gorgie. Neuf tmoins initialement l'avaient identifi comme l'auteur du coup de feu, mais sept ont depuis chang leur version. La Cour suprme avait accord une nouvelle audience en 2009. Davis est soutenu par de nombreuses personnalits et une ptition de 663 000 signatures. Son excution par injection ltale est prvue le 21 septembre 2011  la prison de Jackson. La France a exprim sa proccupation.\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "response_start = prediction.find(assistant_start)\n",
    "\n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start:])\n",
    "print(len(prediction[response_start:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, set=\"test\"):\n",
    "    summaries = [data_point['summary'] for data_point in dataset[set]]\n",
    "    predictions = []\n",
    "\n",
    "    for data_point in dataset[set]:\n",
    "        prompt = prepare_prompt(data_point, summary_included=False)\n",
    "        encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(\n",
    "                input_ids=encoding.input_ids,\n",
    "                attention_mask=encoding.attention_mask,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            \n",
    "        prediction = tokenizer_slm.decode(output[0], skip_special_tokens=True)\n",
    "        response_start = prediction.find(assistant_start)\n",
    "        # print(f\"response start {response_start}\")\n",
    "        predictions.append(prediction[response_start:])\n",
    "    # print(f\"predictions {predictions}\")\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=summaries)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=summaries, lang=\"fr\")\n",
    "    \n",
    "    print(f\"set = {set} : ROUGE Scores: {rouge_results} BERTScore: {bert_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    ")\n",
    "model_raw.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataset, set)\u001b[0m\n\u001b[1;32m      7\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer_slm(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m tokenizer_slm\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m response_start \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mfind(assistant_start)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:276\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    279\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m-> 1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_hooks\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(model_raw, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

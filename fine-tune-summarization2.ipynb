{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl evaluate matplotlib tensorboard rouge_score bert_score\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "from utils import prepare_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEE A DEV:\n",
    "-lire papiers de recherches\n",
    "-Tester modele encodeur decodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = load_dataset('json', data_files='dataset_llm_generated.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_raw.select_columns([\"text\", \"summary\"])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distibution')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_word(example):\n",
    "    return {\"num_word\": len(example[\"summary\"].split())}\n",
    "\n",
    "dataset_num_word = dataset['train'].map(get_num_word)\n",
    "\n",
    "plt.hist(dataset_num_word[\"num_word\"], bins=100)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Total\")\n",
    "plt.title(\"Distibution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/tiktoken/load.py:154\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     token, rank \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    155\u001b[0m     ret[base64\u001b[38;5;241m.\u001b[39mb64decode(token)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rank)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1722\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1622\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1622\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1624\u001b[0m         [\n\u001b[1;32m   1625\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1626\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1627\u001b[0m         ]\n\u001b[1;32m   1628\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1615\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1615\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1616\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1591\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[0;32m-> 1591\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1592\u001b[0m byte_encoder \u001b[38;5;241m=\u001b[39m bytes_to_unicode()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/tiktoken/load.py:157\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken_bpe_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mValueError\u001b[0m: Error parsing line b'\\x0e' in /Data/gabriel-mercier/slm_models/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/spiece.model",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m slm_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/mt5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenizer_slm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Data/gabriel-mercier/slm_models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer_slm\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer_slm\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:944\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    942\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         )\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2292\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2292\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2294\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2297\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:119\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_ids \u001b[38;5;241m=\u001b[39m extra_ids\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 139\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1727\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1723\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1724\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1725\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1729\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1731\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "slm_name = \"google/mt5-base\"\n",
    "#slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer_slm = AutoTokenizer.from_pretrained(slm_name, cache_dir=\"/Data/gabriel-mercier/slm_models\")\n",
    "tokenizer_slm.pad_token = tokenizer_slm.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "summary_num_tokens = 200\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer_slm(examples[\"text\"], truncation=True, max_length=max_length)\n",
    "    targets = tokenizer_slm(examples[\"summary\"], truncation=True, max_length=summary_num_tokens)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_train_temp = tokenized_datasets[\"train\"].train_test_split(test_size=0.4, seed=42)\n",
    "split_valid_test = split_train_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": split_train_temp[\"train\"],        \n",
    "    \"validation\": split_valid_test[\"train\"],      \n",
    "    \"test\": split_valid_test[\"test\"]              \n",
    "})\n",
    "\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 896)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#lora_alpha = 2 * rank\n",
    "attn_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "mlp_modules = [\"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(r=16, \n",
    "                         lora_alpha=32,\n",
    "                         target_modules=attn_modules,\n",
    "                         lora_dropout=0.05,\n",
    "                         bias='none',\n",
    "                         task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model_raw, lora_config)\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2162688 || all params: 317282176 || trainable%: 0.6816292132338376\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résume précisément le texte suivant en français en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. Évite les phrases inutiles et reformule les idées clairement.\n",
      "\n",
      "Texte :\n",
      "Troy Davis est présenté par de nombreuses personnalités comme le prototype du Noir innocent, condamné à mort pour le meurtre d'un policier blanc en 1989. AP/Thibault Camus Plus de trois cent rassemblements ont été organisés vendredi 16 septembre aux Etats-Unis et dans le monde pour réclamer la clémence pour Troy Davis, un condamné à mort qui doit être exécuté mercredi, dont le sort est désormais entre les mains du comité des grâces de Géorgie. L'exécution de Troy Davis, symbole international de la lutte contre la peine de mort, a été programmée par injection létale mercredi à la prison de Jackson, malgré les doutes sur sa culpabilité. Le comité des grâces de Géorgie est appelé lundi à commuer ou non la peine capitale de Davis en prison à vie. Une pétition réunissant plus de 663 000 signatures réclamant la grâce de Davis lui a été remise jeudi, selon ses soutiens. \"Il y a trop de doutes dans cette affaire, nous espérons qu'ils entendront le message: est-on sûr que nous n'allons pas exécuter un innocent ?\", a déclaré Laura Moye, directrice de la campagne pour l'abolition de la peine de mort à Amnesty international. A Paris, environ 150 manifestants, dont une grande majorité vêtue de tee-shirts arborant le portrait de Troy Davis, se sont réunis à l'appel de plusieurs organisations. Aux Etats-Unis, treize rassemblements étaient organisés en fin d'après-midi aux quatre coins du pays. A Chicago, devant le siège de campagne de Barack Obama, environ 200 personnes se sont rassemblées, scandant \"Libérez Troy Davis !\". Dans la capitale fédérale Washington, une centaine de personnes ont manifesté habillés de tee-shirts bleus \"Justice pour Troy Davis\" et portant des pancartes \"Fin de l'Etat meurtrier\", \"La peine de mort doit cesser\", ou \"Le couloir de la mort de Georgie est raciste\". Troy Davis a été exécuté le 21 septembre 2011, par injection létale au pénitencier de Jackson, en Géorgie. AFP/HO LA FRANCE EXPRIME SA \"PRÉOCCUPATION\" Agé de 42 ans, dont 20 passés dans le couloir de la mort en Géorgie, Troy Davis est présenté par de nombreuses personnalités comme le prototype du Noir innocent, condamné à mort pour le meurtre d'un policier blanc en 1989. Il a été soutenu par des personnalités comme Jimmy Carter, le Pape Benoît XVI ou l'actrice Susan Sarandon. La France pour sa part a exprimé vendredi sa \"préoccupation\". Neuf témoins ont désigné à l'époque Troy Davis comme l'auteur du coup de feu mais l'arme du crime n'a jamais été retrouvée et aucune empreinte digitale ou ADN n'a été relevée. Depuis, sept témoins sont revenus sur leurs déclarations, dont certains ont désigné un autre tireur. La Cour suprême avait offert à Troy Davis la possibilité exceptionnelle, en août 2009, de bénéficier d'une nouvelle audience. Plusieurs témoins avaient raconté, sans convaincre le juge fédéral, comment la police les avait persuadés à l'époque de désigner le jeune Noir.\n",
      "\n",
      "Résumé concis et structuré (100 mots maximum) :\n"
     ]
    }
   ],
   "source": [
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_start = \"Résumé concis et structuré (100 mots maximum) :\"\n",
    "\n",
    "def example1():\n",
    "    \n",
    "\n",
    "    encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "    start_index = prediction.find(assistant_start)\n",
    "    if start_index != -1:\n",
    "        response_start = start_index + len(assistant_start)\n",
    "    else:\n",
    "        response_start = 0 \n",
    "\n",
    "    print(\"=== GENERATED SUMMARY ===\")\n",
    "    print(prediction[response_start:])\n",
    "    print(len(prediction[response_start:].split()))\n",
    "    print(\"=== LABEL SUMMARY ===\")\n",
    "    print(summary_data)\n",
    "    print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prepare_prompt(data_point)+tokenizer_slm.eos_token \n",
    "    #print(f\"full_prompt {full_prompt}\")\n",
    "    tokenized_full_prompt = tokenizer_slm(full_prompt, return_tensors='pt')\n",
    "    labels = tokenized_full_prompt.input_ids.clone() ## FILL THE GAP: create the labels first by cloning input_ids\n",
    "    \n",
    "    # prompt = full_prompt[:full_prompt.find(\"Résumé\")] + \"Résumé\"\n",
    "    \n",
    "    assistant_token = tokenizer_slm(\"Résumé concis et structuré\", return_tensors='pt')['input_ids'][0]\n",
    "    T = tokenized_full_prompt['input_ids'].flatten()\n",
    "    S = assistant_token.flatten()\n",
    "    \n",
    "    for i in range(len(T) - len(S) + 1):\n",
    "        if torch.equal(T[i:i+len(S)], S):\n",
    "            end_prompt_idx = i+len(S)   ## FILL THE GAP: get the index of the '<assistant>:' (or the equivalent token) in order to replace all but response tokens with -100\n",
    "    labels[:, :end_prompt_idx] = -100\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_split[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_val = dataset_split[\"validation\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_test = DatasetDict({\n",
    "    \"test\": dataset_split[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"L'amnistie aura lieu avant la visite du président [Thein Sein] en Inde\" mercredi, a assuré un responsable briman, lundi. AP/Khin Maung Win La télévision d'Etat en Birmanie annonce, mardi 11 octobre, la libération de plus de 6 300 \"prisonniers\", sans préciser si des détenus politiques comptaient parmi les bénéficiaires mais alors que les annonces en ce sens se sont multipliées dans le pays ces derniers jours. Les premières libérations interviendront mercredi. La commission nationale pour les droits de l'homme, mise en place en septembre par le gouvernement, avait réclamé, quelques heures auparavant, dans un quotidien officiel la libération des \"prisonniers de conscience\" afin de répondre aux appels en ce sens de la communauté internationale. Dans une lettre ouverte publiée par le quotidien anglophone New Light of Myanmar, considéré comme le porte-parole du régime, la commission rappelle mardi que \"le secrétaire général des Nations unies et un certain nombre de pays réclament la libération de ce qu'ils décrivent comme des 'prisonniers de conscience'\". La commission \"demande humblement au président, en gage de sa magnanimité, d'amnistier ces prisonniers et de les libérer de prison\", a conclu son président, Win Mya, dans une rare reconnaissance de l'existence de prisonniers politiques dans le pays. Lundi, des responsables gouvernementaux avaient déjà indiqué qu'une amnistie incluant des prisonniers politiques aurait lieu dans les jours suivants. \"L'amnistie aura lieu avant la visite du président [Thein Sein] en Inde\" mercredi, a assuré un responsable. Les Etats-Unis, l'Union européenne et l'opposition démocratique birmane réclament la libération de quelque deux mille prisonniers politiques – militants politiques, avocats, artistes, journalistes – pour prouver la sincérité des réformes politiques actuelles. La mesure est aussi considérée comme une condition sine qua non pour envisager la levée des sanctions économiques et politiques en place depuis la fin des années 1990.\n",
      "La télévision d'État birmane annonce la libération prochaine de plus de 6 300 prisonniers. Les premières libérations interviendront mercredi, avant la visite du président Thein Sein en Inde. La commission nationale des droits de l'homme demande l'amnistie des \"prisonniers de conscience\". Des responsables gouvernementaux ont confirmé une amnistie incluant des prisonniers politiques. Les États-Unis, l'UE et l'opposition birmane réclament la libération de quelque 2 000 prisonniers politiques pour prouver la sincérité des réformes. Cette mesure est vue comme une condition pour lever les sanctions économiques et politiques en place depuis 1990. Le texte met pour la première fois en lumière l'existence de prisonniers politiques en Birmanie. Résumé concis et structuré (100 mots maximum) :\n",
      "[84836, 31323, 132971, 41525, 512, 67967, 45832, 517, 662, 54367, 662, 220, 16, 15, 15, 77099, 7192, 13, 61161, 265, 4686, 72, 1729, 3541, 3501, 3956, 22396, 2010, 15510, 25437, 2676, 294, 6, 453, 83896, 12788, 409, 3980, 17276, 13, 28024, 85, 632, 3541, 31747, 304, 332, 3658, 1842, 14836, 1111, 3541, 877, 13700, 1185, 11998, 478, 382, 1178, 68, 6260, 73630, 57491, 64759, 645, 39042, 38281, 32570, 1187, 74736, 3845, 88702, 508, 785, 258, 1345, 258, 60, 662, 2263, 68, 1, 4704, 837, 8579, 11, 264, 1071, 324, 963, 650, 76832, 1411, 39210, 11, 326, 55898, 13, 10106, 32854, 41557, 11331, 2185, 12190, 4929, 42016, 13013, 294, 6, 31860, 266, 662, 36819, 1515, 645, 45220, 11, 296, 36389, 220, 16, 16, 18491, 37608, 11, 1187, 3051, 52201, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 330, 649, 3335, 77, 4813, 497, 15510, 50525, 12059, 4403, 939, 7439, 1960, 355, 3354, 8303, 469, 51591, 1167, 1346, 8155, 3541, 83133, 69, 24024, 3861, 9870, 44475, 1709, 3541, 75739, 662, 3761, 6097, 511, 14789, 7299, 500, 72, 13700, 6866, 512, 21241, 26652, 35752, 4813, 48201, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 13, 4929, 12123, 6995, 1574, 4914, 3541, 96122, 409, 326, 6, 86613, 11, 56359, 662, 1992, 662, 94643, 1346, 512, 84082, 39180, 11, 49490, 9333, 564, 309, 963, 11, 44789, 75045, 97444, 277, 402, 517, 11, 6866, 650, 98468, 3591, 2786, 13029, 1187, 3051, 52201, 939, 330, 649, 3335, 77, 4813, 409, 41463, 1, 53301, 409, 74771, 265, 10047, 906, 2010, 662, 3761, 6097, 409, 1187, 141032, 2590, 37035, 13, 45606, 6185, 69456, 5908, 64832, 77411, 7888, 1346, 512, 98468, 3591, 6454, 385, 4844, 1532, 8658, 315, 52355, 11, 76454, 67762, 21572, 512, 59072, 56998, 1263, 3845, 137269, 11, 1187, 12123, 60058, 6712, 296, 36389, 1709, 330, 273, 511, 129532, 70137, 88075, 939, 19140, 650, 550, 1842, 650, 3654, 12736, 409, 21241, 9333, 564, 2838, 1187, 3051, 52201, 409, 3761, 922, 84117, 7439, 5082, 80441, 21572, 939, 364, 649, 3335, 77, 4813, 409, 41463, 6, 3263, 4929, 12123, 330, 82931, 68, 38512, 478, 7906, 88702, 11, 662, 342, 424, 409, 822, 8455, 276, 98381, 11, 294, 57491, 64759, 1268, 26652, 9343, 77, 4813, 1842, 409, 3541, 3051, 51324, 409, 9343, 497, 264, 92124, 84, 4438, 88702, 11, 12190, 3017, 64, 11, 6866, 6185, 8848, 87505, 409, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 6866, 512, 21241, 13, 444, 55898, 11, 939, 4200, 4788, 84082, 39180, 11981, 83664, 1167, 45839, 1257, 75266, 922, 30009, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 43421, 1315, 38281, 6866, 3541, 48201, 45832, 1783, 13, 330, 43, 57491, 64759, 645, 39042, 38281, 32570, 1187, 74736, 3845, 88702, 508, 785, 258, 1345, 258, 60, 662, 2263, 68, 1, 4704, 837, 8579, 11, 264, 1071, 324, 963, 650, 76832, 13, 11615, 18888, 1862, 82245, 285, 11, 326, 6, 32658, 140927, 1842, 326, 6, 453, 3487, 78541, 72532, 2372, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 25552, 296, 4517, 9343, 77, 4813, 3354, 8303, 1365, 38349, 3354, 8303, 11, 1822, 509, 1862, 11, 10049, 288, 11, 22825, 288, 1365, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 3354, 8303, 1160, 64732, 13, 4929, 83821, 1788, 27363, 76454, 13763, 7888, 21572, 6185, 2971, 57668, 73525, 2477, 4914, 84675, 1409, 1187, 22638, 7888, 939, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 1187, 1875, 939, 64738, 220, 16, 24, 24, 15, 382, 84836, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 14512, 8747, 42016, 13013, 294, 6, 135212, 15248, 1515, 68, 45220, 1187, 3051, 52201, 462, 331, 8346, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 9343, 77, 4813, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 11, 32570, 1187, 74736, 3845, 88702, 576, 258, 1345, 258, 662, 2263, 68, 13, 4929, 12123, 6995, 1574, 939, 96122, 409, 326, 6, 86613, 61063, 326, 57491, 64759, 645, 939, 330, 649, 3335, 77, 4813, 409, 41463, 3263, 3874, 4200, 4788, 84082, 39180, 11981, 14508, 7683, 963, 6185, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 13, 11615, 28024, 83, 1862, 82245, 285, 11, 326, 6, 2230, 1842, 326, 6, 453, 3487, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 220, 17, 220, 15, 15, 15, 9343, 77, 4813, 3354, 8303, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 13, 61308, 83821, 1788, 47134, 21572, 6185, 2971, 4914, 27505, 3541, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 220, 16, 24, 24, 15, 13, 1967, 67967, 2270, 4914, 1187, 54033, 36191, 662, 141096, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 662, 36819, 1515, 645, 13, 50123, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 549, 151645]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 320, 16, 15, 15, 77099, 7192, 8, 14512, 8747, 42016, 13013, 294, 6, 135212, 15248, 1515, 68, 45220, 1187, 3051, 52201, 462, 331, 8346, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 9343, 77, 4813, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 11, 32570, 1187, 74736, 3845, 88702, 576, 258, 1345, 258, 662, 2263, 68, 13, 4929, 12123, 6995, 1574, 939, 96122, 409, 326, 6, 86613, 61063, 326, 57491, 64759, 645, 939, 330, 649, 3335, 77, 4813, 409, 41463, 3263, 3874, 4200, 4788, 84082, 39180, 11981, 14508, 7683, 963, 6185, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 13, 11615, 28024, 83, 1862, 82245, 285, 11, 326, 6, 2230, 1842, 326, 6, 453, 3487, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 220, 17, 220, 15, 15, 15, 9343, 77, 4813, 3354, 8303, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 13, 61308, 83821, 1788, 47134, 21572, 6185, 2971, 4914, 27505, 3541, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 220, 16, 24, 24, 15, 13, 1967, 67967, 2270, 4914, 1187, 54033, 36191, 662, 141096, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 662, 36819, 1515, 645, 13, 50123, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 549, 151645]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[1]['text'])\n",
    "print(dataset_train[1]['summary'])\n",
    "print(dataset_train[1]['input_ids'])\n",
    "print(dataset_train[1]['labels'])\n",
    "print(dataset_train[1]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46484375 Memory used before training (GB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved()/1024**3, 'Memory used before training (GB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46484375 Memory used before training (GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/lib64/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.544400</td>\n",
       "      <td>1.449584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.384800</td>\n",
       "      <td>1.416431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>1.413173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.671875 Memory used after training (GB)\n",
      "14.671875 Memory used before training (GB)\n",
      "train length 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.536200</td>\n",
       "      <td>1.362942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.347800</td>\n",
       "      <td>1.360146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.219800</td>\n",
       "      <td>1.359396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.537109375 Memory used after training (GB)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import transformers\n",
    "\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=2,\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=45,   \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "\n",
    "def train_on_subset(fraction):\n",
    "    global model\n",
    "    #shuffle and sample fraction\n",
    "    subset_train = dataset_train.shuffle().select(range(int(len(dataset_train)*fraction)))\n",
    "    subset_val = dataset_val.shuffle().select(range(int(len(dataset_val)*fraction)))\n",
    "    print('train length', len(subset_train))\n",
    "\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=subset_train,\n",
    "        eval_dataset=subset_val,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer_slm, model=model),\n",
    "        )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "repeat = 2\n",
    "for _ in range(repeat):    \n",
    "    print(torch.cuda.memory_reserved()/1024**3 , 'Memory used before training (GB)') \n",
    "    train_on_subset(0.02)\n",
    "    #torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "    print(torch.cuda.memory_reserved()/1024**3 , 'Memory used after training (GB)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.537109375  (GB) Memory used during training\n",
      "0.529296875  (GB) Memory used during training\n",
      "Model Parameters Memory Usage: 438.71826171875 MB\n",
      "Total gradient memory usage: 0 MB\n",
      "Dataset size: 3.4196739196777344 MB\n",
      "5.742784\n",
      "5.743312\n",
      "41.115352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.memory_reserved()/1024**3, ' (GB) Memory used during training')\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved()/1024**3, ' (GB) Memory used during training')\n",
    "print(f\"Model Parameters Memory Usage: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 ** 2} MB\")\n",
    "# Track gradient memory usage during the backward pass\n",
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_size = param.grad.element_size() * param.grad.numel() / 1024 ** 2  # in MB\n",
    "        total += grad_size\n",
    "        #print(f\"{name} - Gradient memory usage: {grad_size} MB\")\n",
    "\n",
    "print(f\"Total gradient memory usage: {total} MB\")\n",
    "print(f\"Dataset size: {sum(len(item['input_ids']) for item in dataset_train) / 1024 ** 2} MB\")\n",
    "\n",
    "from pympler import asizeof\n",
    "\n",
    "# Get the full memory usage of the variable (including all referenced objects)\n",
    "\n",
    "print(asizeof.asizeof(model)/10**6)  # In bytes\n",
    "print(asizeof.asizeof(model.named_parameters())/10**6)  # In bytes\n",
    "print(asizeof.asizeof(dataset)/10**6)  # In bytes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SUMMARY ===\n",
      "Résumé concis et structuré (100 mots maximum) : Troy Davis, condamné à mort pour le meurtre d'un policier blanc en 1989, a été présenté par plusieurs personnalités comme le prototype du Noir innocent. Le comité des grâces de Géorgie a appelé lundi à commuer ou non la peine capitale de Davis en prison à vie. Des rassemblements ont eu lieu en Californie, en Amérique du Nord et en Europe. Troy Davis a été exécuté le 21 septembre 2011, après avoir été condamné par l'armée américaine. Les forces de l'ordre ont recueilli des témoins confirmant son arrestation. Le juge fédéral a donné son accord pour une nouvelle audition, mais Davis ne s'est pas rendu à l'épisode. La France a exprimé sa préoccupation face à des doutes sur sa culpabilité. Les ras\n",
      "130\n",
      "=== LABEL SUMMARY ===\n",
      "Troy Davis, condamné à mort pour le meurtre d'un policier en 1989, doit être exécuté malgré des doutes sur sa culpabilité. Des manifestations mondiales réclament sa clémence. Le sort de Davis, 42 ans, est entre les mains du comité des grâces de Géorgie. Neuf témoins initialement l'avaient identifié comme l'auteur du coup de feu, mais sept ont depuis changé leur version. La Cour suprême avait accordé une nouvelle audience en 2009. Davis est soutenu par de nombreuses personnalités et une pétition de 663 000 signatures. Son exécution par injection létale est prévue le 21 septembre 2011 à la prison de Jackson. La France a exprimé sa préoccupation.\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "response_start = prediction.find(assistant_start)\n",
    "\n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start:])\n",
    "print(len(prediction[response_start:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, set=\"test\"):\n",
    "    summaries = [data_point['summary'] for data_point in dataset[set]]\n",
    "    predictions = []\n",
    "\n",
    "    for data_point in dataset[set]:\n",
    "        prompt = prepare_prompt(data_point, summary_included=False)\n",
    "        encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(\n",
    "                input_ids=encoding.input_ids,\n",
    "                attention_mask=encoding.attention_mask,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            \n",
    "        prediction = tokenizer_slm.decode(output[0], skip_special_tokens=True)\n",
    "        response_start = prediction.find(assistant_start)\n",
    "        # print(f\"response start {response_start}\")\n",
    "        predictions.append(prediction[response_start:])\n",
    "    # print(f\"predictions {predictions}\")\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=summaries)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=summaries, lang=\"fr\")\n",
    "    \n",
    "    print(f\"set = {set} : ROUGE Scores: {rouge_results} BERTScore: {bert_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    ")\n",
    "model_raw.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataset, set)\u001b[0m\n\u001b[1;32m      7\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer_slm(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m tokenizer_slm\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m response_start \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mfind(assistant_start)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:276\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    279\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m-> 1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_hooks\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(model_raw, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

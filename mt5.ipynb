{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from transformers import DataCollatorForSeq2Seq, AutoTokenizer, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import prepare_prompt, print_trainable_parameters\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_raw = load_dataset('json', data_files='dataset_llm_generated.json')\n",
    "dataset = dataset_raw.select_columns([\"text\", \"summary\"])\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 4426\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset['train'] = dataset['train'].filter(lambda x: len(x['text'].split()) <= 2500)\n",
    "dataset['train'] = dataset['train'].filter(lambda x: len(x['text'].split()) >= 200)\n",
    "dataset['train'] = dataset['train'].filter(lambda x: len(x['summary'].split()) >= 40)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 2655\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 885\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 886\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_train_temp = dataset[\"train\"].train_test_split(test_size=0.4, seed=42)\n",
    "\n",
    "split_valid_test = split_train_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": split_train_temp[\"train\"],        \n",
    "    \"validation\": split_valid_test[\"train\"],      \n",
    "    \"test\": split_valid_test[\"test\"]              \n",
    "})\n",
    "\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/users/eleves-b/2022/gabriel.mercier/INF_CV/myenv/lib64/python3.9/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"/Data/gabriel-mercier/slm_models\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\", cache_dir=cache_dir)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "model_raw = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\", \n",
    "                                              cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config,\n",
    "                                              device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_finetune = True\n",
    "\n",
    "if lora_finetune:\n",
    "    lora_config = LoraConfig(r=16, \n",
    "                            lora_alpha=32,\n",
    "                            target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "                            lora_dropout=0.05,\n",
    "                            bias='none',\n",
    "                            task_type=\"SEQ_2_SEQ_LM\")\n",
    "\n",
    "    model = get_peft_model(model_raw, lora_config)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3538944 || all params: 505724160 || trainable%: 0.6997775229880258\n"
     ]
    }
   ],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.do_sample = True\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "R√©sume pr√©cis√©ment le texte suivant en fran√ßais en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. √âvite les phrases inutiles et reformule les id√©es clairement.\n",
      "\n",
      "Texte :\n",
      "Le 24 ao√ªt 1991, des Moscovites accompagnent les d√©pouilles des trois victimes du putsch, tu√©es trois jours plus t√¥t. Gueorgui Pinkhassov/Magnum C‚Äô√©tait encore le temps des vacances, du repos dans les datchas. Le major-colonel du KGB Valeri Chiria√Øev dormait dans la sienne, une bicoque sans eau, quelque part dans un trou perdu de la r√©gion de Iaroslav, quand la ¬´ bo√Æte ¬ª fix√©e au mur s‚Äôest mise √† gr√©siller. ¬´ On ne savait m√™me plus si ce machin marchait encore ¬ª, raconte-t-il. L‚Äôantique haut-parleur avait √©t√© install√© dans toutes les demeures sovi√©tiques pour pr√©venir d‚Äôune guerre nucl√©aire. Le message contenait aussi un code secret pour toutes les forces de s√©curit√©. Et ce matin du 19 ao√ªt 1991, il y a vingt-cinq ans, Valeri Chiria√Øev d√©couvre, stup√©fait, qu‚Äôun coup d‚ÄôEtat est en cours √† Moscou, men√© par les plus conservateurs des dirigeants. L‚Äôhomme de la glasnost Oppos√©s √† la signature, pr√©vue le lendemain, d‚Äôun nouvel accord d‚Äôassociation entre les membres d‚Äôune URSS r√©form√©e et rebaptis√©e ¬´ Union des r√©publiques socialistes souveraines ¬ª et non plus ¬´ sovi√©tiques ¬ª, huit hauts dignitaires tentent d‚Äô√©carter du pouvoir Mikha√Øl Gorbatchev, l‚Äôhomme de la glasnost (la transparence), ¬´ retenu ¬ª dans sa r√©sidence d‚Äô√©t√©, √† Foros, en Crim√©e. L‚Äôinitiative tourne court : en cinq jours, les conspirateurs r√©unis dans un √©ph√©m√®re Comit√© pour l‚Äô√©tat d‚Äôurgence, dont faisait partie Vladimir Krioutchkov, le patron du KGB, sont arr√™t√©s. Le putsch des nostalgiques d‚Äôun pouvoir communiste fort a √©chou√©. Ces √©v√©nements vont pr√©cipiter la chute de l‚Äôempire. Le Parti communiste de l‚ÄôUnion sovi√©tique (PCUS) est dissous, la statue de ¬≠F√©lix Dzerjinski, le fondateur de la Tch√©ka, la police bolchevique √† l‚Äôorigine du KGB, d√©boulonn√©e, et quatre mois plus tard, le 26 d√©cembre 1991, √† Minsk, la Russie, la Bi√©lorussie et l‚ÄôUkraine signent officiellement l‚Äôacte de d√©c√®s de l‚ÄôURSS. M√©dus√©, le monde assiste √† l‚Äôeffondrement sans violence d‚Äôune des grandes puissances de l‚Äôapr√®s-guerre, et √† l‚Äô√©croulement d‚Äôun r√©gime totalitaire. Article r√©serv√© √† nos abonn√©s Lire aussi Dans la Russie de Poutine, l‚Äôanniversaire ignor√© du putsch rat√© de 1991 Le vingt-cinqui√®me anniversaire du putsch rat√© n‚Äôa donn√© lieu √† aucune r√©jouissance en Russie. Aucune comm√©moration officielle n‚Äôa √©t√© organis√©e, ni m√™me un discours prononc√©, bien que Vladimir Poutine tire directement son pouvoir de ces jours fi√©vreux o√π tout a bascul√©. Si Boris Eltsine n‚Äô√©tait pas mont√© sur un char pour haranguer la foule et s‚Äôopposer aux putschistes devant le parlement de la ¬≠R√©publique socialiste sovi√©tique de Russie, l‚Äôactuel pr√©sident russe n‚Äôaurait sans doute jamais acc√©d√© √† la plus haute marche de l‚ÄôEtat. Un quart de si√®cle plus tard, c‚Äôest en Crim√©e, la p√©ninsule ukrainienne annex√©e en 2014, que Vladimir Poutine a choisi de se rendre √† la date du 19 ao√ªt.\n",
      "\n",
      "R√©sum√© concis et structur√© (100 mots maximum) :\n",
      "=== GENERATED SUMMARY ===\n",
      "<extra_id_0> √† l‚Äôint√©rieur.\n",
      "3\n",
      "=== LABEL SUMMARY ===\n",
      "Le 19 ao√ªt 1991, huit conservateurs tentent un coup d'√âtat contre Mikha√Øl Gorbatchev, retenue en Crim√©e. Ils s'opposent √† un nouvel accord r√©formateur. Apr√®s 5 jours, les putschistes sont arr√™t√©s. Cet √©chec pr√©cipite la chute de l'URSS : dissolution du PCUS, effondrement du r√©gime totalitaire. Le 26 d√©cembre 1991, l'URSS dispara√Æt officiellement. En 2016, Vladimir Poutine, qui tire son pouvoir de cet √©pisode, c√©l√®bre le 25e anniversaire en Crim√©e, annex√©e en 2014.\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "assistant_start = \"R√©sum√© concis et structur√© (100 mots maximum) :\"\n",
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print('=== PROMPT ===')\n",
    "print(prompt)\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = -1 \n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start+1:])\n",
    "print(len(prediction[response_start+1:].split()))\n",
    "\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples[\"summary\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad533dd8abae4bffaf0428247e9f2d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e552d261d384f7c9912ea9e46ba5a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/885 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2655\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 885\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_split[\"train\"].map(preprocess_function)\n",
    "dataset_val = dataset_split[\"validation\"].map(preprocess_function)\n",
    "\n",
    "dataset_train = dataset_train.remove_columns([\"text\", \"summary\"])\n",
    "dataset_val = dataset_val.remove_columns([\"text\", \"summary\"])\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a data collator for seq2seq tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/gabriel.mercier/INF_CV/myenv/lib64/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"./mt5_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train.select(range(50)),\n",
    "    eval_dataset=dataset_val.select(range(10)),\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.760200</td>\n",
       "      <td>2.792951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.445600</td>\n",
       "      <td>2.770083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=3.6252304216225943, metrics={'train_runtime': 35.0735, 'train_samples_per_second': 4.277, 'train_steps_per_second': 1.026, 'total_flos': 42347121868800.0, 'train_loss': 3.6252304216225943, 'epoch': 2.8})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./encoder_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./autoregressive_model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "R√©sume pr√©cis√©ment le texte suivant en fran√ßais en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. √âvite les phrases inutiles et reformule les id√©es clairement.\n",
      "\n",
      "Texte :\n",
      "Le 24 ao√ªt 1991, des Moscovites accompagnent les d√©pouilles des trois victimes du putsch, tu√©es trois jours plus t√¥t. Gueorgui Pinkhassov/Magnum C‚Äô√©tait encore le temps des vacances, du repos dans les datchas. Le major-colonel du KGB Valeri Chiria√Øev dormait dans la sienne, une bicoque sans eau, quelque part dans un trou perdu de la r√©gion de Iaroslav, quand la ¬´ bo√Æte ¬ª fix√©e au mur s‚Äôest mise √† gr√©siller. ¬´ On ne savait m√™me plus si ce machin marchait encore ¬ª, raconte-t-il. L‚Äôantique haut-parleur avait √©t√© install√© dans toutes les demeures sovi√©tiques pour pr√©venir d‚Äôune guerre nucl√©aire. Le message contenait aussi un code secret pour toutes les forces de s√©curit√©. Et ce matin du 19 ao√ªt 1991, il y a vingt-cinq ans, Valeri Chiria√Øev d√©couvre, stup√©fait, qu‚Äôun coup d‚ÄôEtat est en cours √† Moscou, men√© par les plus conservateurs des dirigeants. L‚Äôhomme de la glasnost Oppos√©s √† la signature, pr√©vue le lendemain, d‚Äôun nouvel accord d‚Äôassociation entre les membres d‚Äôune URSS r√©form√©e et rebaptis√©e ¬´ Union des r√©publiques socialistes souveraines ¬ª et non plus ¬´ sovi√©tiques ¬ª, huit hauts dignitaires tentent d‚Äô√©carter du pouvoir Mikha√Øl Gorbatchev, l‚Äôhomme de la glasnost (la transparence), ¬´ retenu ¬ª dans sa r√©sidence d‚Äô√©t√©, √† Foros, en Crim√©e. L‚Äôinitiative tourne court : en cinq jours, les conspirateurs r√©unis dans un √©ph√©m√®re Comit√© pour l‚Äô√©tat d‚Äôurgence, dont faisait partie Vladimir Krioutchkov, le patron du KGB, sont arr√™t√©s. Le putsch des nostalgiques d‚Äôun pouvoir communiste fort a √©chou√©. Ces √©v√©nements vont pr√©cipiter la chute de l‚Äôempire. Le Parti communiste de l‚ÄôUnion sovi√©tique (PCUS) est dissous, la statue de ¬≠F√©lix Dzerjinski, le fondateur de la Tch√©ka, la police bolchevique √† l‚Äôorigine du KGB, d√©boulonn√©e, et quatre mois plus tard, le 26 d√©cembre 1991, √† Minsk, la Russie, la Bi√©lorussie et l‚ÄôUkraine signent officiellement l‚Äôacte de d√©c√®s de l‚ÄôURSS. M√©dus√©, le monde assiste √† l‚Äôeffondrement sans violence d‚Äôune des grandes puissances de l‚Äôapr√®s-guerre, et √† l‚Äô√©croulement d‚Äôun r√©gime totalitaire. Article r√©serv√© √† nos abonn√©s Lire aussi Dans la Russie de Poutine, l‚Äôanniversaire ignor√© du putsch rat√© de 1991 Le vingt-cinqui√®me anniversaire du putsch rat√© n‚Äôa donn√© lieu √† aucune r√©jouissance en Russie. Aucune comm√©moration officielle n‚Äôa √©t√© organis√©e, ni m√™me un discours prononc√©, bien que Vladimir Poutine tire directement son pouvoir de ces jours fi√©vreux o√π tout a bascul√©. Si Boris Eltsine n‚Äô√©tait pas mont√© sur un char pour haranguer la foule et s‚Äôopposer aux putschistes devant le parlement de la ¬≠R√©publique socialiste sovi√©tique de Russie, l‚Äôactuel pr√©sident russe n‚Äôaurait sans doute jamais acc√©d√© √† la plus haute marche de l‚ÄôEtat. Un quart de si√®cle plus tard, c‚Äôest en Crim√©e, la p√©ninsule ukrainienne annex√©e en 2014, que Vladimir Poutine a choisi de se rendre √† la date du 19 ao√ªt.\n",
      "\n",
      "R√©sum√© concis et structur√© (100 mots maximum) :\n",
      "=== GENERATED SUMMARY ===\n",
      "<extra_id_0> de l‚ÄôURSS. Texte : <extra_id_1> le texte en fran√ßais. Texte en fran√ßais. Texte en fran√ßais. L‚Äôhistoire..  <extra_id_2>...... .. .. . . . . . . . . . . . . . . . . . / ... / / / ' ' / ' ' l  s s r r s n s t s l'  <extra_id_40> l '  <extra_id_41>  <extra_id_41>  <extra_id_24> '  <extra_id_41> T l '  <extra_id_41>  <extra_id_39> <extra_id_39>  <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39>\n",
      "122\n",
      "=== LABEL SUMMARY ===\n",
      "Le 19 ao√ªt 1991, huit conservateurs tentent un coup d'√âtat contre Mikha√Øl Gorbatchev, retenue en Crim√©e. Ils s'opposent √† un nouvel accord r√©formateur. Apr√®s 5 jours, les putschistes sont arr√™t√©s. Cet √©chec pr√©cipite la chute de l'URSS : dissolution du PCUS, effondrement du r√©gime totalitaire. Le 26 d√©cembre 1991, l'URSS dispara√Æt officiellement. En 2016, Vladimir Poutine, qui tire son pouvoir de cet √©pisode, c√©l√®bre le 25e anniversaire en Crim√©e, annex√©e en 2014.\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "assistant_start = \"R√©sum√© concis et structur√© (100 mots maximum) :\"\n",
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print('=== PROMPT ===')\n",
    "print(prompt)\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = -1 \n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start+1:])\n",
    "print(len(prediction[response_start+1:].split()))\n",
    "\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    summaries = [data_point['summary'] for data_point in dataset]\n",
    "    predictions = []\n",
    "\n",
    "    for data_point in tqdm(dataset):\n",
    "        prompt = prepare_prompt(data_point, summary_included=False)\n",
    "        encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(\n",
    "                input_ids=encoding.input_ids,\n",
    "                attention_mask=encoding.attention_mask,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            \n",
    "        prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response_start = prediction.find(assistant_start)\n",
    "        predictions.append(prediction[response_start:])\n",
    "\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=summaries)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=summaries, lang=\"fr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_results['precision'])\n",
    "    bert_recall = np.mean(bert_results['recall'])\n",
    "    bert_f1 = np.mean(bert_results['f1'])\n",
    "\n",
    "    print(f\"BERTScore - Precision: {bert_precision:.4f}, Recall: {bert_recall:.4f}, F1: {bert_f1:.4f}\")\n",
    "    print(f\"ROUGEScores - {rouge_results}\")\n",
    "    print('\\n')\n",
    "    \n",
    "    return rouge_results, {'Precision':bert_precision, 'Recall':bert_recall, 'F1':bert_f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_1 = dataset_test.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:23<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.5700, Recall: 0.4051, F1: 0.4732\n",
      "ROUGEScores - {'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "rouges_results_finetune, bert_results_finetune = evaluate_model(model, dataset_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_finetune = {\n",
    "    \"rouge\": rouges_results_finetune,\n",
    "    \"bert\": bert_results_finetune\n",
    "}\n",
    "\n",
    "with open(\"mt5_evaluation_results_finetune.json\", \"w\") as f:\n",
    "    json.dump(results_finetune, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:20<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.6403, Recall: 0.4503, F1: 0.5285\n",
      "ROUGEScores - {'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouges_results_raw, bert_results_raw = evaluate_model(model_raw, dataset_test_1)\n",
    "\n",
    "results_raw = {\n",
    "    \"rouge\": rouges_results_raw,\n",
    "    \"bert\": bert_results_raw\n",
    "}\n",
    "\n",
    "with open(\"mt5_evaluation_results_raw.json\", \"w\") as f:\n",
    "    json.dump(results_raw, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

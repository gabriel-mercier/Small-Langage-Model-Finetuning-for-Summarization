{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from transformers import DataCollatorForSeq2Seq, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import prepare_prompt, print_trainable_parameters, evaluate_model\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = load_from_disk('dataset_split')\n",
    "\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer_slm = AutoTokenizer.from_pretrained(slm_name, cache_dir=\"/Data/gabriel-mercier/slm_models\", padding_side=\"left\")\n",
    "tokenizer_slm.pad_token = tokenizer_slm.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=16, \n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias='none',\n",
    "                        task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model_raw, lora_config)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.do_sample = True\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_start = \"Résumé concis et structuré (100 mots maximum) :\"\n",
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "\n",
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = -1 \n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start+1:])\n",
    "print(len(prediction[response_start+1:].split()))\n",
    "\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prepare_prompt(data_point)+tokenizer_slm.eos_token \n",
    "    tokenized_full_prompt = tokenizer_slm(full_prompt, return_tensors='pt')\n",
    "    labels = tokenized_full_prompt.input_ids.clone() \n",
    "    \n",
    "    assistant_token = tokenizer_slm(\"Résumé concis et structuré\", return_tensors='pt')['input_ids'][0]\n",
    "   \n",
    "    complement_token = tokenizer_slm(\"(100 mots maximum) :\", return_tensors='pt')['input_ids'][0]\n",
    "    \n",
    "    T = tokenized_full_prompt['input_ids'].flatten()\n",
    "    S = assistant_token.flatten()\n",
    "    \n",
    "    for i in range(len(T) - len(S) + 1):\n",
    "        if torch.equal(T[i:i+len(S)], S):\n",
    "            end_prompt_idx = i+len(S)   \n",
    "    \n",
    "    labels[:, :end_prompt_idx+len(complement_token)] = -100\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_split[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_val = dataset_split[\"validation\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_test = dataset_split[\"test\"]\n",
    "\n",
    "dataset_train = dataset_train.remove_columns([\"text\", \"summary\"])\n",
    "dataset_val = dataset_val.remove_columns([\"text\", \"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train)\n",
    "print(dataset_val)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer_slm, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = -1 \n",
    "\n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start+1:])\n",
    "print(len(prediction[response_start+1:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouges_results_finetune, bert_results_finetune = evaluate_model(model, dataset_test, tokenizer_slm, device, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_finetune = {\n",
    "    \"rouge\": rouges_results_finetune,\n",
    "    \"bert\": bert_results_finetune\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results_finetune.json\", \"w\") as f:\n",
    "    json.dump(results_finetune, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    ")\n",
    "model_raw.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouges_results_raw, bert_results_raw = evaluate_model(model, dataset_test, tokenizer_slm, device, generation_config)\n",
    "\n",
    "results_raw = {\n",
    "    \"rouge\": rouges_results_raw,\n",
    "    \"bert\": bert_results_raw\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results_raw.json\", \"w\") as f:\n",
    "    json.dump(results_raw, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

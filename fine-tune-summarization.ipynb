{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEE A DEV:\n",
    "-lire papiers de recherches\n",
    "-Tester modele encodeur decodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e041f49f8b4c7487c1d8faa530dc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_raw = load_dataset('json', data_files='dataset_llm_generated.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_raw)\n",
    "\n",
    "dataset = dataset_raw.select_columns([\"text\", \"summary\"])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c638e81c3c84293867bece8e392d8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distibution')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_word(example):\n",
    "    return {\"num_word\": len(example[\"summary\"].split())}\n",
    "\n",
    "dataset_num_word = dataset['train'].map(get_num_word)\n",
    "\n",
    "plt.hist(dataset_num_word[\"num_word\"], bins=100)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Total\")\n",
    "plt.title(\"Distibution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c5b772677c4d889a86e4770beaab96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1a008cb066433f962380cf530d4ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1870230449844c7aa2e1c0c09b494bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5cbe9379b04434b876e7fef461bff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer_slm = AutoTokenizer.from_pretrained(slm_name, cache_dir=\"/Data/gabriel-mercier/slm_models\")\n",
    "tokenizer_slm.pad_token = tokenizer_slm.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1518ede95d1484c941721b5d02564d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "summary_num_tokens = 200\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer_slm(examples[\"text\"], truncation=True, max_length=max_length)\n",
    "    targets = tokenizer_slm(examples[\"summary\"], truncation=True, max_length=summary_num_tokens)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_train_temp = tokenized_datasets[\"train\"].train_test_split(test_size=0.4, seed=42)\n",
    "\n",
    "split_valid_test = split_train_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": split_train_temp[\"train\"],        \n",
    "    \"validation\": split_valid_test[\"train\"],      \n",
    "    \"test\": split_valid_test[\"test\"]              \n",
    "})\n",
    "\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bf7bffe87246d7a35233e681115c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8647bbf0adb435283ccb2e390ad8250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01334648507e416ab814f20c6fc3265c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora_alpha = 2 * rank\n",
    "lora_config = LoraConfig(r=8, \n",
    "                         lora_alpha=16,\n",
    "                         target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                         lora_dropout=0.05,\n",
    "                         bias='none',\n",
    "                         task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model_raw, lora_config)\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4325376 || all params: 319444864 || trainable%: 1.3540289694562126\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génère uniquement un résumé concis et précis en 130 mots du texte suivant en français :\n",
      "\n",
      "La page d'accueil du réseau social Facebook. AFP/KAREN BLEIER Se déconnecter ne suffit pas : d'après les observations de l'Australien Nik Cubrilovic, spécialiste en sécurité informatique, Facebook continue d'enregistrer des informations sur ses utilisateurs après leur déconnexion du service. Selon M. Cubrilovic, lorsque l'utilisateur clique sur le bouton \"se déconnecter\" de Facebook, le site laisse sur son ordinateur un fichier qui contient des informations personnelles et continue à communiquer à Facebook des éléments sur la navigation de l'internaute. Pour personnaliser les pages de sites Web, Facebook, comme de nombreux sites ou services, utilise un petit fichier, dit \"cookie\", déposé sur le disque dur de l'ordinateur, et dans lequel sont stockées des informations sur l'identité et la navigation de l'internaute. Ce fonctionnement est normal : ce qui l'est moins, note M. Cubrilovic, c'est que lorsque l'internaute se déconnecte, le cookie n'est pas effacé, mais simplement modifié. L'utilisateur qui continue à surfer transmet ainsi, sans le savoir, des informations à Facebook ; et le cookie, qui reste sur la machine, conserve des informations à son sujet. \"Si vous vous connectez à Facebook depuis un ordinateur public, et que vous cliquez sur 'se déconnecter', vous laissez malgré tout derrière vous des empreintes digitales. D'après ce que je constate, ces empreintes restent présentes jusqu'à ce que quelqu'un supprime manuellement tous les cookies Facebook de l'ordinateur\", écrit M. Cubrilovic. En réponse à l'article de M. Cubrilovic, Gregg Stefancik, un ingénieur de Facebook, explique que \"les cookies de Facebook ne sont pas utilisés pour espionner les internautes. Ce n'est tout simplement pas leur rôle. En revanche, nous utilisons ces cookies pour fournir du contenu personnalisé (...), améliorer notre service (...) ou protéger nos utilisateurs et notre service (par exemple pour nous protéger d'attaques par déni de service ou en demandant une deuxième authentification lorsque l'utilisateur se connecte depuis un endroit inhabituel).\" CONTROVERSES SUR LES NOUVELLES FONCTIONNALITÉS Ce débat sur l'utilisation des cookies par Facebook intervient quelques jours après la présentation de nouvelles fonctionnalités par Mark Zuckerberg, le fondateur de Facebook, lors de la conférence F8 du réseau social. Parmi ces nouvelles fonctionnalités, la possibilité pour certains services de publier automatiquement des informations sur le profil des utilisateurs – par exemple, les morceaux qu'ils sont en train d'écouter sur des services comme Deezer ou Spotify. Le chercheur Dave Winer extrapole dans un exemple imaginaire, celui d'un mafioso qui utiliserait Facebook : \"'Bull Mancuso vient de lire un article expliquant comment tuer un autre mafieux'. Bull n'a pas commenté l'article. Il n'a pas cliqué sur le bouton j'aime. Il s'est juste rendu sur une page Web. Et une annonce a été faite en son nom à toutes les personnes qui le suivent sur Facebook.\" \"Ces applications demandent explicitement la permission des utilisateurs avant de publier des informations, rétorque, sur Hacker News, un autre ingénieur de Facebook. Je comprends que vous puissiez être inquiet si, par exemple, vous avez installé cette application sans savoir qu'elle pourrait publier des messages. Mais si cela vous pose des problèmes par rapport à votre vie privée, vous pouvez toujours changer les permissions accordées à cette application dans les paramètres de votre compte.\"\n",
      "\n",
      "Voici le résumé de 130 mots :\n"
     ]
    }
   ],
   "source": [
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SUMMARY ===\n",
      "Voici le résumé de 130 mots : \n",
      "\n",
      " La page d'accueil de Facebook fournit des fichiers de cookies qui continuent de s'accumuler sur l'ordinateur des utilisateurs, permettant à Facebook de profiter de leurs données personnelles. Ces fichiers, datant de l'abandon de l'utilisateur, sont conservés sur le disque dur et peuvent être récupérés sans leur savoir. Les cookie sont utilisés pour personnaliser les sites Web, améliorer le service et protéger les utilisateurs contre les attaques. La nouvelle fonctionnalité d'affichage automatique de l'information sur le profil des utilisateurs, selon un chercheur, peut causer des préoccupations concernant la privauté des utilisateurs. Les autorités et les ingénieurs de Facebook exiguent explicitement que les utilisateurs respectent ces permis avant d'utiliser des applications. \n",
      "\n",
      "(150 mots) \n",
      "\n",
      "Note: J'ai noté que le texte original mention\n",
      "126\n",
      "=== LABEL SUMMARY ===\n",
      " Selon Nik Cubrilovic, spécialiste en sécurité informatique, Facebook continue de collecter des données sur ses utilisateurs même après leur déconnexion, grâce à des cookies persistants. Ces fichiers stockent des informations personnelles et continuent à communiquer avec Facebook. En réponse, Facebook affirme que ces cookies servent à personnaliser le contenu, améliorer le service et protéger les utilisateurs, et non à espionner. Par ailleurs, Facebook a introduit de nouvelles fonctionnalités permettant aux applications tierces de publier automatiquement des informations sur les profils des utilisateurs, avec leur consentement préalable. Cependant, des experts s'inquiètent des implications pour la vie privée. \n",
      "\n",
      "(149 mots) \n",
      "\n",
      "Note: J'ai inclus les nouvelles fonctionnalités mentionnées dans le texte original pour donner un contexte plus complet, tout en respectant la limite de mots. Si\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "response_start = prediction.find(assistant_start)\n",
    "\n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start:])\n",
    "print(len(prediction[response_start:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prepare_prompt(data_point)+tokenizer_slm.eos_token \n",
    "    #print(f\"full_prompt {full_prompt}\")\n",
    "    tokenized_full_prompt = tokenizer_slm(full_prompt, return_tensors='pt')\n",
    "    labels = tokenized_full_prompt.input_ids.clone() ## FILL THE GAP: create the labels first by cloning input_ids\n",
    "    \n",
    "    # prompt = full_prompt[:full_prompt.find(\"Résumé\")] + \"Résumé\"\n",
    "    \n",
    "    assistant_token = tokenizer_slm(\"Voici le résumé de\", return_tensors='pt')['input_ids'][0]\n",
    "    T = tokenized_full_prompt['input_ids'].flatten()\n",
    "    S = assistant_token.flatten()\n",
    "    \n",
    "    for i in range(len(T) - len(S) + 1):\n",
    "        if torch.equal(T[i:i+len(S)], S):\n",
    "            end_prompt_idx = i+len(S)   ## FILL THE GAP: get the index of the '<assistant>:' (or the equivalent token) in order to replace all but response tokens with -100\n",
    "    labels[:, :end_prompt_idx] = -100\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544a29b5444743d3afcab6fa53748cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815ffab75ccb473a9eefa4f8da999870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset_train = dataset_split[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_val = dataset_split[\"validation\"].shuffle(seed=42).map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florian Philippot, tête de liste du Front national, vient voter au bureau de vote n° 4 de Forbach (Moselle), au Lycée Jean Moulin. JULIEN DANIEL / MYOP POUR \"LE MONDE\" Les sondages le prédisaient, les urnes l’ont confirmé. Florian Philippot, tête de liste Front national (FN) en Alsace-Champagne-Ardenne-Lorraine, termine en tête du premier tour des élections régionales, dimanche 6 décembre. Le Front national, loin devant la droite Selon les résultats définitifs, la liste de Florian Philippot arrive en tête avec 36,06 % des voix. Philippe Richert, le candidat de la droite et du centre, est relégué plus de 10 points derrière, avec 25,83 % des voix, tandis que le socialiste, Jean-Pierre Masseret, ne réunit que 16,11 % des bulletins. L’écologiste Sandrine Bélier obtient quant à elle entre 6,7 % des voix, le régionaliste Jean-Georges Trouillet 4,78 %, Laurent Jacobelli (Debout la France) 4,78 %, Patrick Peron (Front de gauche) 3,07 %. Au Palais des congrès de Strasbourg, dans une salle réservée par le FN, les quelques militants frontistes présents ont exulté devant un écran de télévision à l’annonce des premières estimations en scandant « Florian, Florian » et en entonnant un début de Marseillaise. « Les résultats sont historiques pour les patriotes. Ce soir s’est exprimé un vote d’amour, un vote d’adhésion, un vote de conviction », a déclaré M. Philippot, avant d’appeler à la mobilisation face au « système politique » : « On va aller chercher quelques grands patrons [pour s’opposer au FN], le star-system sera contre nous mais vous n’aurez plus peur. » Le FN continue à progresser dans la région Lors des dernières élections régionales, en 2010, les différents candidats frontistes n’avaient pas dépassé les 16 % au premier tour (15,89 % en Champagne-Ardenne, 14,87 % en Lorraine, 13,49 % en Alsace). Et leur score avait baissé au second tour. Au soir des élections européennes de 2014, la liste de Florian Philippot avait déjà terminé en tête et obtenu 28,98 % des suffrages. Cette fois-ci, l’avance du FN est encore plus importante. Et sans doute difficile à rattraper. Le PS pourrait se maintenir au second tour Largement distancé dans la région Alsace-Champagne-Ardenne, le Parti socialiste local s’oriente pour le moment vers le maintien de sa liste. Dimanche soir, la tête de liste, Jean-Pierre Masseret, a répété dès le début de la soirée qu’il ne changerait pas sa stratégie décidée depuis plusieurs semaines : « Il n’y a pas de retrait et pas de fusion qui n’aurait aucun sens », a-t-il déclaré à la presse depuis son QG de Maizières-lès-Metz avant d’ajouter : « Vous me voyez ? Eh bien, vous me verrez la semaine prochaine. » Les têtes de listes départementales socialistes doivent se réunir, lundi matin, pour discuter. « Il doit rester des forces progressistes au sein du conseil régional. Rien n’est joué et il est encore possible de mobiliser au second tour », espère Joëlle Barat, tête de liste socialiste dans les Ardennes. Mais certaines voix discordantes commencent à se faire entendre. Le maire socialiste de Strasbourg, Roland Ries, a ainsi appelé dimanche soir au retrait de la liste. « Nous partageons [avec Philippe Richert, le candidat de la droite et du centre] les valeurs de la République, ce qui n’est pas le cas avec (le candidat FN) Florian Philippot (...) J’appelle en mon âme et conscience au retrait de la liste PS et au vote contre le Front national. » Mathieu Klein, président socialiste du conseil départemental de Meurthe-et-Moselle, est sur la même ligne : « Le maintien en l’état de la liste de gauche est un potentiel marchepied pour l’élection de M. Philippot et notre priorité est de faire battre le Front national. » Sandrine Bélier, tête de liste Europe-écologie-Les Verts, a elle appelé à une coalition des partis contre le Front national. Philippe Richert ne semble lui pas opposé à une fusion. Il a demandé à chacun de prendre ses responsabilités mais a exigé à ce que des décisions soient prises vite car les listes doivent être déposées, mardi 8 décembre, en préfecture. Forte hausse de la participation Dans le Grand Est, la participation globale a connu une forte progression, même si plus de la moitié des électeurs ne se sont pas déplacés (52,07 %) avec une progression de près de 8 à 10 points dans le Bas-Rhin ou dans les Vosges. La mobilisation de l’entre-deux tours pourrait aussi avoir une influence sur l’issue du scrutin. Lors des élections municipales à Forbach en 2014, le score de M. Philippot avait stagné lors du second tour (35,17 % contre 35,74 % au premier tour) alors que le maire socialiste, Laurent Kalinowski, avait conservé son siège de maire en remontant douze points lors d’une quadrangulaire. La société civile s’était alors fortement opposée à la candidature du numéro 2 du Front national. Reste à savoir à quel point le climat électoral a changé depuis cette époque. « Il ne faut pas que nos concitoyens se trompent d’enjeu. Nous sommes ce soir la seule liste susceptible de présenter une alternative républicaine », a déclaré, Philippe Richert. Nos dernières informations sur les résultats en Alsace-Champagne-Ardenne-Lorraine :\n",
      " Au premier tour des élections régionales dans le Grand Est, Florian Philippot, tête de liste du Front National, arrive en tête avec 36,06% des voix, devançant largement la droite (25,83%) et le PS (16,11%). Ces résultats marquent une forte progression du FN par rapport aux précédentes élections. Le PS envisage de se maintenir au second tour, malgré des appels au retrait pour empêcher le FN de gagner. La participation a augmenté de manière significative, mais reste inférieure à 50%. Les discussions sur des alliances pour contrer le FN se poursuivent. \n",
      "\n",
      "(147 mots) \n",
      "\n",
      "Note: J'ai légèrement ajusté le nombre de mots pour respecter la consigne tout en gardant le contenu essentiel. Le texte original contient des informations supplémentaires qui n'ont pas été\n",
      "[38, 10394, 12068, 28021, 1114, 650, 9333, 1242, 963, 3529, 285, 1842, 132971, 662, 220, 16, 18, 15, 77099, 3845, 67967, 45832, 517, 662, 54367, 14512, 3882, 22090, 19945, 354, 11, 81967, 409, 32924, 3845, 14902, 5313, 11, 85000, 25323, 7906, 43302, 409, 6910, 308, 11616, 220, 19, 409, 1752, 34538, 320, 76306, 6712, 701, 7906, 444, 3337, 7888, 19685, 72834, 3732, 13, 87060, 40, 953, 83410, 9061, 608, 18224, 3067, 393, 6791, 330, 867, 28537, 1150, 1, 11615, 46837, 1134, 512, 26293, 4243, 64, 1167, 11, 3541, 65867, 288, 326, 527, 544, 7683, 963, 13, 96056, 19945, 354, 11, 81967, 409, 32924, 14902, 5313, 320, 41604, 8, 662, 44462, 578, 29553, 1121, 23870, 6691, 81, 5183, 811, 8125, 90022, 11, 4647, 482, 662, 81967, 3845, 20496, 7216, 939, 3958, 67679, 37582, 82423, 11, 5103, 27997, 220, 21, 134829, 13, 1967, 14902, 5313, 11, 76960, 76458, 1187, 6973, 632, 23204, 263, 3541, 99146, 46271, 2327, 21835, 11, 1187, 32924, 409, 96056, 19945, 354, 17331, 662, 81967, 9753, 220, 18, 21, 11, 15, 21, 1018, 939, 4069, 941, 13, 66854, 8107, 529, 11, 512, 5685, 266, 409, 1187, 6973, 632, 1842, 3845, 12261, 11, 1788, 1351, 963, 136418, 5519, 409, 220, 16, 15, 3501, 2694, 64120, 11, 9753, 220, 17, 20, 11, 23, 18, 1018, 939, 4069, 941, 11, 53567, 285, 1709, 512, 3590, 16776, 11, 19685, 9299, 44423, 9167, 261, 295, 11, 834, 9333, 3843, 1709, 220, 16, 21, 11, 16, 16, 1018, 939, 17432, 1330, 13, 444, 527, 19667, 16155, 68, 8677, 39001, 425, 963, 5623, 4514, 1167, 10272, 3784, 26451, 9281, 220, 21, 11, 22, 1018, 939, 4069, 941, 11, 512, 37582, 3914, 16776, 19685, 12, 9499, 80775, 41682, 40884, 220, 19, 11, 22, 23, 1018, 11, 63221, 24521, 20508, 320, 1912, 72901, 1187, 9625, 8, 220, 19, 11, 22, 23, 1018, 11, 19626, 3616, 263, 320, 23395, 409, 95621, 8, 220, 18, 11, 15, 22, 18524, 32050, 10925, 2782, 939, 30169, 12142, 409, 4509, 83150, 11, 6866, 6185, 70532, 30707, 648, 7888, 1346, 512, 49487, 11, 3541, 44789, 38349, 4065, 74360, 29079, 805, 14508, 505, 494, 963, 76458, 650, 3958, 71985, 409, 42016, 13013, 3784, 326, 527, 97623, 939, 6811, 58207, 14932, 804, 662, 54292, 517, 12486, 96056, 11, 96056, 8182, 1842, 662, 1197, 26378, 517, 650, 64285, 409, 386, 2583, 483, 4016, 13, 12486, 11615, 99146, 14789, 6990, 8303, 4914, 3541, 28225, 6295, 13, 23769, 56892, 274, 21402, 15169, 318, 963, 650, 6910, 294, 76038, 413, 11, 650, 6910, 294, 527, 51587, 5397, 290, 11, 650, 6910, 409, 28091, 64701, 264, 7439, 12821, 963, 386, 13, 19945, 354, 11, 32570, 294, 81075, 7865, 3784, 1187, 28805, 7923, 3579, 7906, 12486, 71501, 77936, 8182, 549, 12486, 1913, 11164, 47808, 22216, 9034, 44789, 93427, 51760, 508, 54519, 274, 527, 453, 20071, 7906, 49487, 1125, 512, 6774, 36648, 34637, 44873, 16890, 9870, 9012, 308, 39653, 552, 89, 5519, 1051, 324, 13, 8182, 1967, 49487, 3060, 3784, 5098, 261, 6866, 1187, 87000, 444, 1087, 939, 35752, 58207, 3958, 67679, 37582, 82423, 11, 662, 220, 17, 15, 16, 15, 11, 3541, 84014, 5685, 1862, 4065, 74360, 308, 527, 2907, 1167, 6368, 7439, 6385, 963, 3541, 220, 16, 21, 1018, 7906, 20496, 7216, 320, 16, 20, 11, 23, 24, 1018, 662, 87539, 6691, 81, 5183, 811, 11, 220, 16, 19, 11, 23, 22, 1018, 662, 444, 90022, 11, 220, 16, 18, 11, 19, 24, 1018, 662, 44462, 578, 568, 18888, 27032, 5456, 49490, 12789, 1038, 963, 7906, 2086, 7216, 13, 32050, 56892, 939, 3958, 67679, 94099, 15209, 409, 220, 17, 15, 16, 19, 11, 1187, 32924, 409, 96056, 19945, 354, 49490, 45839, 10204, 963, 662, 81967, 1842, 4514, 1481, 220, 17, 23, 11, 24, 23, 1018, 939, 8489, 81, 1134, 13, 61308, 36191, 62384, 11, 326, 56792, 681, 3845, 49487, 1788, 34497, 5519, 35797, 13, 18888, 15510, 294, 2133, 76311, 3784, 435, 2991, 3191, 13, 1967, 11405, 91115, 511, 4981, 268, 404, 7906, 2086, 7216, 444, 858, 1114, 1582, 1129, 963, 6866, 1187, 87000, 44462, 578, 29553, 1121, 23870, 6691, 81, 5183, 811, 11, 512, 3660, 72, 3590, 16776, 2205, 274, 527, 269, 12844, 4914, 512, 4445, 5436, 512, 4981, 3591, 409, 822, 32924, 13, 8126, 27997, 56892, 11, 1187, 81967, 409, 32924, 11, 19685, 9299, 44423, 9167, 261, 295, 11, 264, 9333, 79, 38783, 86351, 512, 64285, 409, 1187, 142434, 922, 43896, 834, 61339, 1315, 6368, 822, 142682, 142440, 7888, 40099, 50754, 5234, 32803, 549, 12486, 7543, 308, 90317, 264, 6368, 409, 312, 29432, 1842, 6368, 409, 36508, 7774, 308, 527, 4110, 1315, 84524, 6097, 64701, 264, 2385, 56275, 7439, 12821, 963, 3784, 1187, 1652, 325, 40099, 4438, 1207, 38, 409, 11331, 33235, 30439, 2852, 12142, 5251, 42189, 32570, 294, 527, 1630, 2676, 549, 12486, 40521, 752, 23063, 10125, 937, 60551, 14370, 11, 9012, 752, 2739, 22741, 1187, 71609, 462, 331, 8346, 13, 8182, 11615, 259, 89176, 409, 1140, 288, 141055, 3831, 40189, 288, 96469, 511, 9333, 359, 404, 11, 326, 55898, 5517, 258, 11, 4914, 2560, 27951, 13, 12486, 7543, 41082, 2732, 261, 939, 8437, 5098, 74360, 7906, 19349, 3845, 29925, 321, 37582, 3914, 13, 431, 3591, 308, 21402, 26970, 963, 1842, 3815, 1788, 34497, 3204, 409, 28805, 12059, 7906, 2086, 7216, 64701, 16541, 12068, 10946, 12179, 65501, 4716, 266, 11, 81967, 409, 32924, 3590, 16776, 6866, 3541, 73907, 15209, 13, 33347, 3654, 288, 4069, 941, 31041, 15477, 1063, 26365, 3784, 511, 19463, 68279, 265, 13, 1967, 7491, 554, 3590, 16776, 409, 4509, 83150, 11, 57806, 431, 550, 11, 264, 43251, 75090, 963, 5103, 27997, 56892, 7906, 312, 29432, 409, 1187, 32924, 13, 12486, 47009, 949, 424, 2382, 508, 87323, 66854, 8107, 529, 11, 512, 5685, 266, 409, 1187, 6973, 632, 1842, 3845, 12261, 60, 3541, 95935, 409, 1187, 50123, 9585, 36426, 11, 3761, 7774, 308, 21402, 6368, 512, 4760, 9753, 320, 273, 5685, 266, 49487, 8, 96056, 19945, 354, 65777, 619, 81075, 6712, 662, 1615, 27905, 2660, 1842, 41463, 7906, 312, 29432, 409, 1187, 32924, 11405, 1842, 7906, 6910, 44873, 512, 14902, 5313, 13, 8182, 4149, 25173, 42141, 11, 88702, 3590, 16776, 3845, 29925, 321, 141055, 278, 409, 2157, 324, 1782, 96010, 5251, 436, 6712, 11, 1788, 1729, 1187, 26486, 26814, 549, 12486, 1967, 4981, 3591, 662, 326, 66182, 266, 409, 1187, 32924, 409, 95621, 1788, 650, 35775, 13029, 15217, 747, 1122, 4914, 326, 32677, 1170, 409, 386, 13, 19945, 354, 1842, 28349, 4867, 12815, 1788, 409, 19463, 14754, 265, 512, 14902, 5313, 13, 8182, 8677, 39001, 425, 963, 5623, 11, 81967, 409, 32924, 4505, 12, 19667, 37369, 8125, 288, 15058, 82, 11, 264, 26451, 75090, 963, 3784, 6185, 25217, 939, 949, 285, 44873, 512, 14902, 5313, 13, 66854, 8107, 529, 834, 96387, 24397, 6368, 7922, 963, 3784, 6185, 36508, 13, 7543, 264, 7479, 963, 3784, 94817, 359, 409, 58294, 15537, 4200, 45400, 5397, 9870, 264, 73672, 963, 3784, 3761, 1709, 939, 34781, 6805, 773, 1167, 548, 4909, 82357, 1803, 3541, 1140, 288, 96469, 22911, 7439, 966, 13700, 11, 296, 36389, 220, 23, 134829, 11, 662, 26293, 3751, 552, 13, 11002, 68, 70272, 325, 409, 1187, 20239, 45606, 512, 10304, 9403, 11, 1187, 20239, 13206, 1574, 264, 4534, 84, 6185, 63958, 32724, 11, 26486, 4403, 5519, 409, 1187, 4544, 12303, 963, 939, 97051, 37547, 834, 511, 14789, 6368, 7439, 500, 580, 5397, 320, 20, 17, 11, 15, 22, 1018, 8, 9753, 6185, 32724, 409, 83264, 409, 220, 23, 3784, 220, 16, 15, 3501, 6866, 512, 14662, 10911, 41557, 5908, 6866, 3541, 647, 436, 4188, 13, 4929, 28805, 7923, 409, 326, 527, 39897, 6810, 2200, 30161, 91115, 27363, 38829, 6185, 10173, 1729, 326, 527, 11159, 3845, 69242, 13, 444, 1087, 939, 3958, 67679, 19615, 3831, 3784, 1752, 34538, 662, 220, 17, 15, 16, 19, 11, 512, 5456, 409, 386, 13, 19945, 354, 49490, 53263, 963, 36853, 3845, 2086, 7216, 320, 18, 20, 11, 16, 22, 1018, 44873, 220, 18, 20, 11, 22, 19, 1018, 7906, 20496, 7216, 8, 44475, 1709, 512, 7491, 554, 3590, 16776, 11, 63221, 26832, 258, 28284, 11, 49490, 11110, 963, 4438, 4403, 72799, 409, 7491, 554, 662, 1299, 544, 517, 24576, 2986, 3501, 36853, 294, 35477, 29136, 524, 73675, 13, 4929, 76724, 18462, 457, 274, 66182, 1315, 44475, 369, 78407, 7922, 7888, 3784, 1187, 5685, 1568, 3845, 97977, 220, 17, 3845, 14902, 5313, 13, 9063, 68, 3784, 46829, 3784, 25025, 1459, 512, 11076, 266, 97051, 9819, 264, 2547, 963, 40099, 20061, 3958, 5368, 591, 13, 12486, 7543, 834, 45990, 6368, 1709, 11891, 3529, 275, 2253, 724, 511, 64536, 45125, 294, 51189, 3756, 84, 13, 47009, 73433, 3761, 56892, 1187, 85706, 32924, 46181, 409, 29079, 1950, 6185, 10555, 9333, 888, 8346, 64701, 264, 7439, 12821, 963, 11, 66854, 8107, 529, 13, 49997, 35752, 58207, 43727, 1729, 3541, 99146, 662, 44462, 578, 29553, 1121, 23870, 6691, 81, 5183, 811, 8125, 90022, 14512, 27515, 3375, 512, 9333, 1242, 963, 409, 220, 16, 18, 15, 77099, 14512, 32050, 20496, 7216, 939, 3958, 67679, 37582, 82423, 6866, 512, 10304, 9403, 11, 96056, 19945, 354, 11, 81967, 409, 32924, 3845, 14902, 5055, 11, 17331, 662, 81967, 9753, 220, 18, 21, 11, 15, 21, 4, 939, 4069, 941, 11, 3483, 276, 3131, 517, 4053, 1114, 1187, 6973, 632, 320, 17, 20, 11, 23, 18, 11334, 1842, 512, 11405, 320, 16, 21, 11, 16, 16, 52072, 61363, 99146, 3594, 16769, 6185, 63958, 32724, 3845, 49487, 1346, 38374, 10047, 140057, 21243, 3958, 67679, 13, 1967, 11405, 84675, 424, 409, 511, 4981, 268, 404, 7906, 2086, 7216, 11, 8641, 89866, 939, 906, 2010, 7906, 312, 29432, 4914, 8486, 5498, 9034, 512, 49487, 409, 342, 39438, 13, 4929, 20239, 264, 48706, 963, 409, 84622, 4595, 1388, 11, 9870, 65067, 4132, 34185, 552, 3784, 220, 20, 15, 14360, 11615, 20333, 1729, 939, 69700, 4914, 6027, 261, 512, 49487, 511, 66707, 84, 80441, 13, 4710, 7, 16, 19, 22, 77099, 8, 4710, 9112, 25, 619, 33055, 143979, 478, 73134, 963, 512, 12736, 409, 77099, 4914, 5091, 261, 1187, 1585, 18857, 16559, 662, 21881, 517, 512, 75254, 3956, 306, 13029, 13, 1967, 67967, 4024, 683, 1167, 939, 43727, 81013, 41525, 17276, 7774, 308, 6, 544, 6368, 23639, 151645]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 220, 16, 18, 15, 77099, 14512, 32050, 20496, 7216, 939, 3958, 67679, 37582, 82423, 6866, 512, 10304, 9403, 11, 96056, 19945, 354, 11, 81967, 409, 32924, 3845, 14902, 5055, 11, 17331, 662, 81967, 9753, 220, 18, 21, 11, 15, 21, 4, 939, 4069, 941, 11, 3483, 276, 3131, 517, 4053, 1114, 1187, 6973, 632, 320, 17, 20, 11, 23, 18, 11334, 1842, 512, 11405, 320, 16, 21, 11, 16, 16, 52072, 61363, 99146, 3594, 16769, 6185, 63958, 32724, 3845, 49487, 1346, 38374, 10047, 140057, 21243, 3958, 67679, 13, 1967, 11405, 84675, 424, 409, 511, 4981, 268, 404, 7906, 2086, 7216, 11, 8641, 89866, 939, 906, 2010, 7906, 312, 29432, 4914, 8486, 5498, 9034, 512, 49487, 409, 342, 39438, 13, 4929, 20239, 264, 48706, 963, 409, 84622, 4595, 1388, 11, 9870, 65067, 4132, 34185, 552, 3784, 220, 20, 15, 14360, 11615, 20333, 1729, 939, 69700, 4914, 6027, 261, 512, 49487, 511, 66707, 84, 80441, 13, 4710, 7, 16, 19, 22, 77099, 8, 4710, 9112, 25, 619, 33055, 143979, 478, 73134, 963, 512, 12736, 409, 77099, 4914, 5091, 261, 1187, 1585, 18857, 16559, 662, 21881, 517, 512, 75254, 3956, 306, 13029, 13, 1967, 67967, 4024, 683, 1167, 939, 43727, 81013, 41525, 17276, 7774, 308, 6, 544, 6368, 23639, 151645]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[1]['text'])\n",
    "print(dataset_train[1]['summary'])\n",
    "print(dataset_train[1]['input_ids'])\n",
    "print(dataset_train[1]['labels'])\n",
    "print(dataset_train[1]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/gabriel.mercier/INF_CV/myenv/lib64/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:25, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.482400</td>\n",
       "      <td>1.582629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>1.525151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.439200</td>\n",
       "      <td>1.508682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.197200</td>\n",
       "      <td>1.514697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.199700</td>\n",
       "      <td>1.524360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.021900</td>\n",
       "      <td>1.529037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.3098369932174683, metrics={'train_runtime': 86.0568, 'train_samples_per_second': 4.648, 'train_steps_per_second': 1.162, 'total_flos': 1059873202620672.0, 'train_loss': 1.3098369932174683, 'epoch': 6.666666666666667})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import transformers\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=2,\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=100,   \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer_slm, model=model),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SUMMARY ===\n",
      "Voici le résumé de 130 mots : \n",
      "\n",
      "Facebook continue d'enregistrer des informations sur ses utilisateurs après leur déconnexion, utilisant des cookies pour fournir du contenu personnalisé et améliorer son service. Ces cookies sont conservés sur le disque dur et peuvent être récupérés par Facebook même si l'utilisateur quitte le site. Cependant, selon Nik Cubrilovic, ces cookies ne sont pas utilisés pour espionner les internautes et sont utilisés pour personnaliser les pages de sites Web et améliorer le service. La conférence F8 de Mark Zuckerberg a souligné la possibilité de publier des informations sur le profil des utilisateurs, comme celle de Bull Mancuso, sur Facebook. Les cookies utilisés sont contrôlés par Facebook et ne permettent pas aux utilisateurs de modifier ces données. Cette nouvelle fonctionnalité pourrait entraîner des conséquences pour la privacy des utilisateurs. \n",
      "\n",
      "(150 mots)Human\n",
      "135\n",
      "=== LABEL SUMMARY ===\n",
      " Selon Nik Cubrilovic, spécialiste en sécurité informatique, Facebook continue de collecter des données sur ses utilisateurs même après leur déconnexion, grâce à des cookies persistants. Ces fichiers stockent des informations personnelles et continuent à communiquer avec Facebook. En réponse, Facebook affirme que ces cookies servent à personnaliser le contenu, améliorer le service et protéger les utilisateurs, et non à espionner. Par ailleurs, Facebook a introduit de nouvelles fonctionnalités permettant aux applications tierces de publier automatiquement des informations sur les profils des utilisateurs, avec leur consentement préalable. Cependant, des experts s'inquiètent des implications pour la vie privée. \n",
      "\n",
      "(149 mots) \n",
      "\n",
      "Note: J'ai inclus les nouvelles fonctionnalités mentionnées dans le texte original pour donner un contexte plus complet, tout en respectant la limite de mots. Si\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "response_start = prediction.find(assistant_start)\n",
    "\n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start:])\n",
    "print(len(prediction[response_start:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, set=\"test\"):\n",
    "    summaries = [data_point['summary'] for data_point in dataset[set]]\n",
    "    predictions = []\n",
    "\n",
    "    for data_point in dataset[set]:\n",
    "        prompt = prepare_prompt(data_point, summary_included=False)\n",
    "        encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(\n",
    "                input_ids=encoding.input_ids,\n",
    "                attention_mask=encoding.attention_mask,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            \n",
    "        prediction = tokenizer_slm.decode(output[0], skip_special_tokens=True)\n",
    "        response_start = prediction.find(assistant_start)\n",
    "        # print(f\"response start {response_start}\")\n",
    "        predictions.append(prediction[response_start:])\n",
    "    # print(f\"predictions {predictions}\")\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=summaries)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=summaries, lang=\"fr\")\n",
    "    \n",
    "    print(f\"set = {set} : ROUGE Scores: {rouge_results} BERTScore: {bert_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set = test : ROUGE Scores: {'rouge1': np.float64(0.3972360494508656), 'rouge2': np.float64(0.12946286374612376), 'rougeL': np.float64(0.2062117850250687), 'rougeLsum': np.float64(0.2445120447934191)} BERTScore: {'precision': [0.6826847791671753, 0.6876481771469116, 0.7118455767631531, 0.7292723655700684, 0.733092188835144, 0.7235287427902222, 0.71564120054245, 0.7365008592605591, 0.7425682544708252, 0.7356541156768799, 0.7506924271583557, 0.6814379692077637, 0.7002245187759399, 0.7292063236236572, 0.7402533292770386, 0.7407881617546082, 0.7481139898300171, 0.7332062721252441, 0.6548534631729126, 0.7249160408973694], 'recall': [0.7127090692520142, 0.7098967432975769, 0.7354943156242371, 0.7482820749282837, 0.7252928018569946, 0.7332981824874878, 0.7482444047927856, 0.735686182975769, 0.7856603860855103, 0.7662780284881592, 0.7567068338394165, 0.674723207950592, 0.7263787984848022, 0.7296074032783508, 0.7705840468406677, 0.747780978679657, 0.7370654940605164, 0.7403802871704102, 0.679458737373352, 0.7291091084480286], 'f1': [0.6973739266395569, 0.698595404624939, 0.723476767539978, 0.7386549115180969, 0.7291716933250427, 0.7283807396888733, 0.7315797805786133, 0.7360933423042297, 0.7635067701339722, 0.750653862953186, 0.7536875605583191, 0.6780639886856079, 0.7130619287490845, 0.7294068336486816, 0.7551142573356628, 0.7442681193351746, 0.7425486445426941, 0.736775815486908, 0.6669292449951172, 0.7270064949989319], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.49.0)'}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    ")\n",
    "model_raw.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set = test : ROUGE Scores: {'rouge1': np.float64(0.4105805212040625), 'rouge2': np.float64(0.13805869811222818), 'rougeL': np.float64(0.21424772375608542), 'rougeLsum': np.float64(0.24828295599987146)} BERTScore: {'precision': [0.668182373046875, 0.660563588142395, 0.7115554809570312, 0.7368109226226807, 0.7616187334060669, 0.7188488245010376, 0.7089061737060547, 0.709980845451355, 0.7649809718132019, 0.7887769937515259, 0.7439485192298889, 0.695293664932251, 0.7531929016113281, 0.7419340014457703, 0.7366881966590881, 0.7212579250335693, 0.7290598750114441, 0.7391754388809204, 0.7382654547691345, 0.7328371405601501], 'recall': [0.6786659955978394, 0.6991459131240845, 0.7447879314422607, 0.7568933963775635, 0.7630525827407837, 0.7352464199066162, 0.742567777633667, 0.7189084887504578, 0.7952483296394348, 0.7779242992401123, 0.74933922290802, 0.6804883480072021, 0.7704300284385681, 0.7515066862106323, 0.7383511066436768, 0.7343348264694214, 0.74338698387146, 0.7419833540916443, 0.7439934611320496, 0.7286376953125], 'f1': [0.673383355140686, 0.6793074011802673, 0.7277925610542297, 0.7467172145843506, 0.762334942817688, 0.7269551753997803, 0.7253466248512268, 0.7144168019294739, 0.7798210978507996, 0.7833130359649658, 0.7466340661048889, 0.6878113150596619, 0.761713981628418, 0.7466896772384644, 0.7375187277793884, 0.7277376055717468, 0.7361537218093872, 0.7405768036842346, 0.7411183714866638, 0.7307313084602356], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.49.0)'}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_raw, dataset_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

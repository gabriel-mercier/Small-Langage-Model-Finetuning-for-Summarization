{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl evaluate matplotlib tensorboard\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "from utils import prepare_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEE A DEV:\n",
    "-lire papiers de recherches\n",
    "-Tester modele encodeur decodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = load_dataset('json', data_files='dataset_llm_generated.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_raw.select_columns([\"text\", \"summary\"])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distibution')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_word(example):\n",
    "    return {\"num_word\": len(example[\"summary\"].split())}\n",
    "\n",
    "dataset_num_word = dataset['train'].map(get_num_word)\n",
    "\n",
    "plt.hist(dataset_num_word[\"num_word\"], bins=100)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Total\")\n",
    "plt.title(\"Distibution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slm_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer_slm = AutoTokenizer.from_pretrained(slm_name, cache_dir=\"/Data/gabriel-mercier/slm_models\")\n",
    "tokenizer_slm.pad_token = tokenizer_slm.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "summary_num_tokens = 200\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer_slm(examples[\"text\"], truncation=True, max_length=max_length)\n",
    "    targets = tokenizer_slm(examples[\"summary\"], truncation=True, max_length=summary_num_tokens)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_train_temp = tokenized_datasets[\"train\"].train_test_split(test_size=0.4, seed=42)\n",
    "\n",
    "split_valid_test = split_train_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": split_train_temp[\"train\"],        \n",
    "    \"validation\": split_valid_test[\"train\"],      \n",
    "    \"test\": split_valid_test[\"test\"]              \n",
    "})\n",
    "\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/bin/python'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 896)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#lora_alpha = 2 * rank\n",
    "attn_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "mlp_modules = [\"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(r=16, \n",
    "                         lora_alpha=32,\n",
    "                         target_modules=attn_modules,\n",
    "                         lora_dropout=0.05,\n",
    "                         bias='none',\n",
    "                         task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model_raw, lora_config)\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2162688 || all params: 317282176 || trainable%: 0.6816292132338376\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer_slm.eos_token_id\n",
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résume précisément le texte suivant en français en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. Évite les phrases inutiles et reformule les idées clairement.\n",
      "\n",
      "Texte :\n",
      "Troy Davis est présenté par de nombreuses personnalités comme le prototype du Noir innocent, condamné à mort pour le meurtre d'un policier blanc en 1989. AP/Thibault Camus Plus de trois cent rassemblements ont été organisés vendredi 16 septembre aux Etats-Unis et dans le monde pour réclamer la clémence pour Troy Davis, un condamné à mort qui doit être exécuté mercredi, dont le sort est désormais entre les mains du comité des grâces de Géorgie. L'exécution de Troy Davis, symbole international de la lutte contre la peine de mort, a été programmée par injection létale mercredi à la prison de Jackson, malgré les doutes sur sa culpabilité. Le comité des grâces de Géorgie est appelé lundi à commuer ou non la peine capitale de Davis en prison à vie. Une pétition réunissant plus de 663 000 signatures réclamant la grâce de Davis lui a été remise jeudi, selon ses soutiens. \"Il y a trop de doutes dans cette affaire, nous espérons qu'ils entendront le message: est-on sûr que nous n'allons pas exécuter un innocent ?\", a déclaré Laura Moye, directrice de la campagne pour l'abolition de la peine de mort à Amnesty international. A Paris, environ 150 manifestants, dont une grande majorité vêtue de tee-shirts arborant le portrait de Troy Davis, se sont réunis à l'appel de plusieurs organisations. Aux Etats-Unis, treize rassemblements étaient organisés en fin d'après-midi aux quatre coins du pays. A Chicago, devant le siège de campagne de Barack Obama, environ 200 personnes se sont rassemblées, scandant \"Libérez Troy Davis !\". Dans la capitale fédérale Washington, une centaine de personnes ont manifesté habillés de tee-shirts bleus \"Justice pour Troy Davis\" et portant des pancartes \"Fin de l'Etat meurtrier\", \"La peine de mort doit cesser\", ou \"Le couloir de la mort de Georgie est raciste\". Troy Davis a été exécuté le 21 septembre 2011, par injection létale au pénitencier de Jackson, en Géorgie. AFP/HO LA FRANCE EXPRIME SA \"PRÉOCCUPATION\" Agé de 42 ans, dont 20 passés dans le couloir de la mort en Géorgie, Troy Davis est présenté par de nombreuses personnalités comme le prototype du Noir innocent, condamné à mort pour le meurtre d'un policier blanc en 1989. Il a été soutenu par des personnalités comme Jimmy Carter, le Pape Benoît XVI ou l'actrice Susan Sarandon. La France pour sa part a exprimé vendredi sa \"préoccupation\". Neuf témoins ont désigné à l'époque Troy Davis comme l'auteur du coup de feu mais l'arme du crime n'a jamais été retrouvée et aucune empreinte digitale ou ADN n'a été relevée. Depuis, sept témoins sont revenus sur leurs déclarations, dont certains ont désigné un autre tireur. La Cour suprême avait offert à Troy Davis la possibilité exceptionnelle, en août 2009, de bénéficier d'une nouvelle audience. Plusieurs témoins avaient raconté, sans convaincre le juge fédéral, comment la police les avait persuadés à l'époque de désigner le jeune Noir.\n",
      "\n",
      "Résumé concis et structuré (100 mots maximum) :\n"
     ]
    }
   ],
   "source": [
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SUMMARY ===\n",
      " \n",
      "- Troy Davis, un Noir innocent condamné à mort pour le meurtre d'un policier blanc, est présenté par de nombreux témoins comme le protégé du Noir innocent, le Black Panther, et l'actrice Susan Sarandon.\n",
      "- Les témoins ont également appris qu'il était accusé de crimes contre l'intégrité et d'avoir été considéré comme un suspect dangereux.\n",
      "- Troisième étape : l'exécution de Troy Davis, symbolique de la lutte contre la peine de mort, a été programmée par injection létale au pénitencier de Jackson, malgré les doutes sur sa culpabilité.\n",
      "- Un comité des grâces de Géorgie a été appelé lundi à commuer ou non la peine capitale de Davis en prison à vie.\n",
      "- Une pétition réunissant plus de 663 000\n",
      "120\n",
      "=== LABEL SUMMARY ===\n",
      "Troy Davis, condamné à mort pour le meurtre d'un policier en 1989, doit être exécuté malgré des doutes sur sa culpabilité. Des manifestations mondiales réclament sa clémence. Le sort de Davis, 42 ans, est entre les mains du comité des grâces de Géorgie. Neuf témoins initialement l'avaient identifié comme l'auteur du coup de feu, mais sept ont depuis changé leur version. La Cour suprême avait accordé une nouvelle audience en 2009. Davis est soutenu par de nombreuses personnalités et une pétition de 663 000 signatures. Son exécution par injection létale est prévue le 21 septembre 2011 à la prison de Jackson. La France a exprimé sa préoccupation.\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "assistant_start = \"Résumé concis et structuré (100 mots maximum) :\"\n",
    "\n",
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = 0 \n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start:])\n",
    "print(len(prediction[response_start:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prepare_prompt(data_point)+tokenizer_slm.eos_token \n",
    "    #print(f\"full_prompt {full_prompt}\")\n",
    "    tokenized_full_prompt = tokenizer_slm(full_prompt, return_tensors='pt')\n",
    "    labels = tokenized_full_prompt.input_ids.clone() ## FILL THE GAP: create the labels first by cloning input_ids\n",
    "    \n",
    "    # prompt = full_prompt[:full_prompt.find(\"Résumé\")] + \"Résumé\"\n",
    "    \n",
    "    assistant_token = tokenizer_slm(\"Résumé concis et structuré\", return_tensors='pt')['input_ids'][0]\n",
    "    T = tokenized_full_prompt['input_ids'].flatten()\n",
    "    S = assistant_token.flatten()\n",
    "    \n",
    "    for i in range(len(T) - len(S) + 1):\n",
    "        if torch.equal(T[i:i+len(S)], S):\n",
    "            end_prompt_idx = i+len(S)   ## FILL THE GAP: get the index of the '<assistant>:' (or the equivalent token) in order to replace all but response tokens with -100\n",
    "    labels[:, :end_prompt_idx] = -100\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_split[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "dataset_val = dataset_split[\"validation\"].shuffle(seed=42).map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"L'amnistie aura lieu avant la visite du président [Thein Sein] en Inde\" mercredi, a assuré un responsable briman, lundi. AP/Khin Maung Win La télévision d'Etat en Birmanie annonce, mardi 11 octobre, la libération de plus de 6 300 \"prisonniers\", sans préciser si des détenus politiques comptaient parmi les bénéficiaires mais alors que les annonces en ce sens se sont multipliées dans le pays ces derniers jours. Les premières libérations interviendront mercredi. La commission nationale pour les droits de l'homme, mise en place en septembre par le gouvernement, avait réclamé, quelques heures auparavant, dans un quotidien officiel la libération des \"prisonniers de conscience\" afin de répondre aux appels en ce sens de la communauté internationale. Dans une lettre ouverte publiée par le quotidien anglophone New Light of Myanmar, considéré comme le porte-parole du régime, la commission rappelle mardi que \"le secrétaire général des Nations unies et un certain nombre de pays réclament la libération de ce qu'ils décrivent comme des 'prisonniers de conscience'\". La commission \"demande humblement au président, en gage de sa magnanimité, d'amnistier ces prisonniers et de les libérer de prison\", a conclu son président, Win Mya, dans une rare reconnaissance de l'existence de prisonniers politiques dans le pays. Lundi, des responsables gouvernementaux avaient déjà indiqué qu'une amnistie incluant des prisonniers politiques aurait lieu dans les jours suivants. \"L'amnistie aura lieu avant la visite du président [Thein Sein] en Inde\" mercredi, a assuré un responsable. Les Etats-Unis, l'Union européenne et l'opposition démocratique birmane réclament la libération de quelque deux mille prisonniers politiques – militants politiques, avocats, artistes, journalistes – pour prouver la sincérité des réformes politiques actuelles. La mesure est aussi considérée comme une condition sine qua non pour envisager la levée des sanctions économiques et politiques en place depuis la fin des années 1990.\n",
      "La télévision d'État birmane annonce la libération prochaine de plus de 6 300 prisonniers. Les premières libérations interviendront mercredi, avant la visite du président Thein Sein en Inde. La commission nationale des droits de l'homme demande l'amnistie des \"prisonniers de conscience\". Des responsables gouvernementaux ont confirmé une amnistie incluant des prisonniers politiques. Les États-Unis, l'UE et l'opposition birmane réclament la libération de quelque 2 000 prisonniers politiques pour prouver la sincérité des réformes. Cette mesure est vue comme une condition pour lever les sanctions économiques et politiques en place depuis 1990. Le texte met pour la première fois en lumière l'existence de prisonniers politiques en Birmanie. Résumé concis et structuré (100 mots maximum) :\n",
      "[84836, 31323, 132971, 41525, 512, 67967, 45832, 517, 662, 54367, 662, 220, 16, 15, 15, 77099, 7192, 13, 61161, 265, 4686, 72, 1729, 3541, 3501, 3956, 22396, 2010, 15510, 25437, 2676, 294, 6, 453, 83896, 12788, 409, 3980, 17276, 13, 28024, 85, 632, 3541, 31747, 304, 332, 3658, 1842, 14836, 1111, 3541, 877, 13700, 1185, 11998, 478, 382, 1178, 68, 6260, 73630, 57491, 64759, 645, 39042, 38281, 32570, 1187, 74736, 3845, 88702, 508, 785, 258, 1345, 258, 60, 662, 2263, 68, 1, 4704, 837, 8579, 11, 264, 1071, 324, 963, 650, 76832, 1411, 39210, 11, 326, 55898, 13, 10106, 32854, 41557, 11331, 2185, 12190, 4929, 42016, 13013, 294, 6, 31860, 266, 662, 36819, 1515, 645, 45220, 11, 296, 36389, 220, 16, 16, 18491, 37608, 11, 1187, 3051, 52201, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 330, 649, 3335, 77, 4813, 497, 15510, 50525, 12059, 4403, 939, 7439, 1960, 355, 3354, 8303, 469, 51591, 1167, 1346, 8155, 3541, 83133, 69, 24024, 3861, 9870, 44475, 1709, 3541, 75739, 662, 3761, 6097, 511, 14789, 7299, 500, 72, 13700, 6866, 512, 21241, 26652, 35752, 4813, 48201, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 13, 4929, 12123, 6995, 1574, 4914, 3541, 96122, 409, 326, 6, 86613, 11, 56359, 662, 1992, 662, 94643, 1346, 512, 84082, 39180, 11, 49490, 9333, 564, 309, 963, 11, 44789, 75045, 97444, 277, 402, 517, 11, 6866, 650, 98468, 3591, 2786, 13029, 1187, 3051, 52201, 939, 330, 649, 3335, 77, 4813, 409, 41463, 1, 53301, 409, 74771, 265, 10047, 906, 2010, 662, 3761, 6097, 409, 1187, 141032, 2590, 37035, 13, 45606, 6185, 69456, 5908, 64832, 77411, 7888, 1346, 512, 98468, 3591, 6454, 385, 4844, 1532, 8658, 315, 52355, 11, 76454, 67762, 21572, 512, 59072, 56998, 1263, 3845, 137269, 11, 1187, 12123, 60058, 6712, 296, 36389, 1709, 330, 273, 511, 129532, 70137, 88075, 939, 19140, 650, 550, 1842, 650, 3654, 12736, 409, 21241, 9333, 564, 2838, 1187, 3051, 52201, 409, 3761, 922, 84117, 7439, 5082, 80441, 21572, 939, 364, 649, 3335, 77, 4813, 409, 41463, 6, 3263, 4929, 12123, 330, 82931, 68, 38512, 478, 7906, 88702, 11, 662, 342, 424, 409, 822, 8455, 276, 98381, 11, 294, 57491, 64759, 1268, 26652, 9343, 77, 4813, 1842, 409, 3541, 3051, 51324, 409, 9343, 497, 264, 92124, 84, 4438, 88702, 11, 12190, 3017, 64, 11, 6866, 6185, 8848, 87505, 409, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 6866, 512, 21241, 13, 444, 55898, 11, 939, 4200, 4788, 84082, 39180, 11981, 83664, 1167, 45839, 1257, 75266, 922, 30009, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 43421, 1315, 38281, 6866, 3541, 48201, 45832, 1783, 13, 330, 43, 57491, 64759, 645, 39042, 38281, 32570, 1187, 74736, 3845, 88702, 508, 785, 258, 1345, 258, 60, 662, 2263, 68, 1, 4704, 837, 8579, 11, 264, 1071, 324, 963, 650, 76832, 13, 11615, 18888, 1862, 82245, 285, 11, 326, 6, 32658, 140927, 1842, 326, 6, 453, 3487, 78541, 72532, 2372, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 25552, 296, 4517, 9343, 77, 4813, 3354, 8303, 1365, 38349, 3354, 8303, 11, 1822, 509, 1862, 11, 10049, 288, 11, 22825, 288, 1365, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 3354, 8303, 1160, 64732, 13, 4929, 83821, 1788, 27363, 76454, 13763, 7888, 21572, 6185, 2971, 57668, 73525, 2477, 4914, 84675, 1409, 1187, 22638, 7888, 939, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 1187, 1875, 939, 64738, 220, 16, 24, 24, 15, 382, 84836, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 14512, 8747, 42016, 13013, 294, 6, 135212, 15248, 1515, 68, 45220, 1187, 3051, 52201, 462, 331, 8346, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 9343, 77, 4813, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 11, 32570, 1187, 74736, 3845, 88702, 576, 258, 1345, 258, 662, 2263, 68, 13, 4929, 12123, 6995, 1574, 939, 96122, 409, 326, 6, 86613, 61063, 326, 57491, 64759, 645, 939, 330, 649, 3335, 77, 4813, 409, 41463, 3263, 3874, 4200, 4788, 84082, 39180, 11981, 14508, 7683, 963, 6185, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 13, 11615, 28024, 83, 1862, 82245, 285, 11, 326, 6, 2230, 1842, 326, 6, 453, 3487, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 220, 17, 220, 15, 15, 15, 9343, 77, 4813, 3354, 8303, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 13, 61308, 83821, 1788, 47134, 21572, 6185, 2971, 4914, 27505, 3541, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 220, 16, 24, 24, 15, 13, 1967, 67967, 2270, 4914, 1187, 54033, 36191, 662, 141096, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 662, 36819, 1515, 645, 13, 50123, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 549, 151645]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 320, 16, 15, 15, 77099, 7192, 8, 14512, 8747, 42016, 13013, 294, 6, 135212, 15248, 1515, 68, 45220, 1187, 3051, 52201, 462, 331, 8346, 409, 5519, 409, 220, 21, 220, 18, 15, 15, 9343, 77, 4813, 13, 11615, 6811, 58207, 3051, 13763, 804, 946, 9971, 408, 9411, 4704, 837, 8579, 11, 32570, 1187, 74736, 3845, 88702, 576, 258, 1345, 258, 662, 2263, 68, 13, 4929, 12123, 6995, 1574, 939, 96122, 409, 326, 6, 86613, 61063, 326, 57491, 64759, 645, 939, 330, 649, 3335, 77, 4813, 409, 41463, 3263, 3874, 4200, 4788, 84082, 39180, 11981, 14508, 7683, 963, 6185, 1079, 64759, 645, 18409, 27574, 939, 9343, 77, 4813, 3354, 8303, 13, 11615, 28024, 83, 1862, 82245, 285, 11, 326, 6, 2230, 1842, 326, 6, 453, 3487, 15248, 1515, 68, 9333, 564, 2838, 1187, 3051, 52201, 409, 77971, 220, 17, 220, 15, 15, 15, 9343, 77, 4813, 3354, 8303, 4914, 548, 14826, 1187, 26241, 963, 1003, 963, 939, 9333, 627, 288, 13, 61308, 83821, 1788, 47134, 21572, 6185, 2971, 4914, 27505, 3541, 23746, 81553, 8303, 1842, 3354, 8303, 662, 1992, 40099, 220, 16, 24, 24, 15, 13, 1967, 67967, 2270, 4914, 1187, 54033, 36191, 662, 141096, 326, 6, 92672, 409, 9343, 77, 4813, 3354, 8303, 662, 36819, 1515, 645, 13, 50123, 1242, 963, 3529, 285, 1842, 2036, 324, 963, 320, 16, 15, 15, 77099, 7192, 8, 549, 151645]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[1]['text'])\n",
    "print(dataset_train[1]['summary'])\n",
    "print(dataset_train[1]['input_ids'])\n",
    "print(dataset_train[1]['labels'])\n",
    "print(dataset_train[1]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/lib64/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/100 00:02 < 01:07, 1.40 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 23.54 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 846.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 32\u001b[0m\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForSeq2Seq(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_slm, model\u001b[38;5;241m=\u001b[39mmodel),\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 807 (2 times), autocast_decorator.<locals>.decorate_autocast at line 44 (2 times), convert_outputs_to_fp32.<locals>.forward at line 819 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:873\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    872\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 873\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LLM project/.venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 23.54 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 846.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import transformers\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=2,\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=100,   \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer_slm, model=model),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SUMMARY ===\n",
      "Voici le résumé de 130 mots : \n",
      "\n",
      "Facebook continue d'enregistrer des informations sur ses utilisateurs après leur déconnexion, utilisant des cookies pour fournir du contenu personnalisé et améliorer son service. Ces cookies sont conservés sur le disque dur et peuvent être récupérés par Facebook même si l'utilisateur quitte le site. Cependant, selon Nik Cubrilovic, ces cookies ne sont pas utilisés pour espionner les internautes et sont utilisés pour personnaliser les pages de sites Web et améliorer le service. La conférence F8 de Mark Zuckerberg a souligné la possibilité de publier des informations sur le profil des utilisateurs, comme celle de Bull Mancuso, sur Facebook. Les cookies utilisés sont contrôlés par Facebook et ne permettent pas aux utilisateurs de modifier ces données. Cette nouvelle fonctionnalité pourrait entraîner des conséquences pour la privacy des utilisateurs. \n",
      "\n",
      "(150 mots)Human\n",
      "135\n",
      "=== LABEL SUMMARY ===\n",
      " Selon Nik Cubrilovic, spécialiste en sécurité informatique, Facebook continue de collecter des données sur ses utilisateurs même après leur déconnexion, grâce à des cookies persistants. Ces fichiers stockent des informations personnelles et continuent à communiquer avec Facebook. En réponse, Facebook affirme que ces cookies servent à personnaliser le contenu, améliorer le service et protéger les utilisateurs, et non à espionner. Par ailleurs, Facebook a introduit de nouvelles fonctionnalités permettant aux applications tierces de publier automatiquement des informations sur les profils des utilisateurs, avec leur consentement préalable. Cependant, des experts s'inquiètent des implications pour la vie privée. \n",
      "\n",
      "(149 mots) \n",
      "\n",
      "Note: J'ai inclus les nouvelles fonctionnalités mentionnées dans le texte original pour donner un contexte plus complet, tout en respectant la limite de mots. Si\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer_slm.decode(outputs[0], skip_special_tokens=True)\n",
    "response_start = prediction.find(assistant_start)\n",
    "\n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start:])\n",
    "print(len(prediction[response_start:].split()))\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, set=\"test\"):\n",
    "    summaries = [data_point['summary'] for data_point in dataset[set]]\n",
    "    predictions = []\n",
    "\n",
    "    for data_point in dataset[set]:\n",
    "        prompt = prepare_prompt(data_point, summary_included=False)\n",
    "        encoding = tokenizer_slm(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(\n",
    "                input_ids=encoding.input_ids,\n",
    "                attention_mask=encoding.attention_mask,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            \n",
    "        prediction = tokenizer_slm.decode(output[0], skip_special_tokens=True)\n",
    "        response_start = prediction.find(assistant_start)\n",
    "        # print(f\"response start {response_start}\")\n",
    "        predictions.append(prediction[response_start:])\n",
    "    # print(f\"predictions {predictions}\")\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=summaries)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=summaries, lang=\"fr\")\n",
    "    \n",
    "    print(f\"set = {set} : ROUGE Scores: {rouge_results} BERTScore: {bert_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set = test : ROUGE Scores: {'rouge1': np.float64(0.3972360494508656), 'rouge2': np.float64(0.12946286374612376), 'rougeL': np.float64(0.2062117850250687), 'rougeLsum': np.float64(0.2445120447934191)} BERTScore: {'precision': [0.6826847791671753, 0.6876481771469116, 0.7118455767631531, 0.7292723655700684, 0.733092188835144, 0.7235287427902222, 0.71564120054245, 0.7365008592605591, 0.7425682544708252, 0.7356541156768799, 0.7506924271583557, 0.6814379692077637, 0.7002245187759399, 0.7292063236236572, 0.7402533292770386, 0.7407881617546082, 0.7481139898300171, 0.7332062721252441, 0.6548534631729126, 0.7249160408973694], 'recall': [0.7127090692520142, 0.7098967432975769, 0.7354943156242371, 0.7482820749282837, 0.7252928018569946, 0.7332981824874878, 0.7482444047927856, 0.735686182975769, 0.7856603860855103, 0.7662780284881592, 0.7567068338394165, 0.674723207950592, 0.7263787984848022, 0.7296074032783508, 0.7705840468406677, 0.747780978679657, 0.7370654940605164, 0.7403802871704102, 0.679458737373352, 0.7291091084480286], 'f1': [0.6973739266395569, 0.698595404624939, 0.723476767539978, 0.7386549115180969, 0.7291716933250427, 0.7283807396888733, 0.7315797805786133, 0.7360933423042297, 0.7635067701339722, 0.750653862953186, 0.7536875605583191, 0.6780639886856079, 0.7130619287490845, 0.7294068336486816, 0.7551142573356628, 0.7442681193351746, 0.7425486445426941, 0.736775815486908, 0.6669292449951172, 0.7270064949989319], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.49.0)'}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "    slm_name,\n",
    "    cache_dir=\"/Data/gabriel-mercier/slm_models\",\n",
    ")\n",
    "model_raw.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set = test : ROUGE Scores: {'rouge1': np.float64(0.4105805212040625), 'rouge2': np.float64(0.13805869811222818), 'rougeL': np.float64(0.21424772375608542), 'rougeLsum': np.float64(0.24828295599987146)} BERTScore: {'precision': [0.668182373046875, 0.660563588142395, 0.7115554809570312, 0.7368109226226807, 0.7616187334060669, 0.7188488245010376, 0.7089061737060547, 0.709980845451355, 0.7649809718132019, 0.7887769937515259, 0.7439485192298889, 0.695293664932251, 0.7531929016113281, 0.7419340014457703, 0.7366881966590881, 0.7212579250335693, 0.7290598750114441, 0.7391754388809204, 0.7382654547691345, 0.7328371405601501], 'recall': [0.6786659955978394, 0.6991459131240845, 0.7447879314422607, 0.7568933963775635, 0.7630525827407837, 0.7352464199066162, 0.742567777633667, 0.7189084887504578, 0.7952483296394348, 0.7779242992401123, 0.74933922290802, 0.6804883480072021, 0.7704300284385681, 0.7515066862106323, 0.7383511066436768, 0.7343348264694214, 0.74338698387146, 0.7419833540916443, 0.7439934611320496, 0.7286376953125], 'f1': [0.673383355140686, 0.6793074011802673, 0.7277925610542297, 0.7467172145843506, 0.762334942817688, 0.7269551753997803, 0.7253466248512268, 0.7144168019294739, 0.7798210978507996, 0.7833130359649658, 0.7466340661048889, 0.6878113150596619, 0.761713981628418, 0.7466896772384644, 0.7375187277793884, 0.7277376055717468, 0.7361537218093872, 0.7405768036842346, 0.7411183714866638, 0.7307313084602356], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.49.0)'}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_raw, dataset_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/ayoub.melliti/LLM project/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import DataCollatorForSeq2Seq, AutoTokenizer, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from utils import print_trainable_parameters, prepare_prompt,  evaluate_model\n",
    "import transformers\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "r=128\n",
    "model_name = \"t5-base\" # \"mt5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_split = load_from_disk('dataset_split')\n",
    "\n",
    "print(dataset_split)\n",
    "\n",
    "cache_dir = \"/Data/gabriel-mercier/slm_models\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "if model_name == \"t5-base\":\n",
    "    final_name = \"google-t5/t5-base\"\n",
    "elif model_name == \"mt5-base\":\n",
    "    final_name = \"google/mt5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_name, cache_dir=cache_dir)\n",
    "\n",
    "# Configure BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                            )\n",
    "\n",
    "# Load model\n",
    "model_raw = AutoModelForSeq2SeqLM.from_pretrained(final_name, \n",
    "                                              cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config,\n",
    "                                              device_map=\"auto\")\n",
    "\n",
    "print(model_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(r=r, \n",
    "                            lora_alpha=2*r,\n",
    "                            target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "                            lora_dropout=0.05,\n",
    "                            bias='none',\n",
    "                            task_type=\"SEQ_2_SEQ_LM\")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model_raw, lora_config)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure generation settings\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.do_sample = True\n",
    "\n",
    "# Print trainable parameters\n",
    "perc_param = print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "Résume précisément le texte suivant en français en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. Évite les phrases inutiles et reformule les idées clairement.\n",
      "\n",
      "Texte :\n",
      "Le 24 août 1991, des Moscovites accompagnent les dépouilles des trois victimes du putsch, tuées trois jours plus tôt. Gueorgui Pinkhassov/Magnum C’était encore le temps des vacances, du repos dans les datchas. Le major-colonel du KGB Valeri Chiriaïev dormait dans la sienne, une bicoque sans eau, quelque part dans un trou perdu de la région de Iaroslav, quand la « boîte » fixée au mur s’est mise à grésiller. « On ne savait même plus si ce machin marchait encore », raconte-t-il. L’antique haut-parleur avait été installé dans toutes les demeures soviétiques pour prévenir d’une guerre nucléaire. Le message contenait aussi un code secret pour toutes les forces de sécurité. Et ce matin du 19 août 1991, il y a vingt-cinq ans, Valeri Chiriaïev découvre, stupéfait, qu’un coup d’Etat est en cours à Moscou, mené par les plus conservateurs des dirigeants. L’homme de la glasnost Opposés à la signature, prévue le lendemain, d’un nouvel accord d’association entre les membres d’une URSS réformée et rebaptisée « Union des républiques socialistes souveraines » et non plus « soviétiques », huit hauts dignitaires tentent d’écarter du pouvoir Mikhaïl Gorbatchev, l’homme de la glasnost (la transparence), « retenu » dans sa résidence d’été, à Foros, en Crimée. L’initiative tourne court : en cinq jours, les conspirateurs réunis dans un éphémère Comité pour l’état d’urgence, dont faisait partie Vladimir Krioutchkov, le patron du KGB, sont arrêtés. Le putsch des nostalgiques d’un pouvoir communiste fort a échoué. Ces événements vont précipiter la chute de l’empire. Le Parti communiste de l’Union soviétique (PCUS) est dissous, la statue de ­Félix Dzerjinski, le fondateur de la Tchéka, la police bolchevique à l’origine du KGB, déboulonnée, et quatre mois plus tard, le 26 décembre 1991, à Minsk, la Russie, la Biélorussie et l’Ukraine signent officiellement l’acte de décès de l’URSS. Médusé, le monde assiste à l’effondrement sans violence d’une des grandes puissances de l’après-guerre, et à l’écroulement d’un régime totalitaire. Article réservé à nos abonnés Lire aussi Dans la Russie de Poutine, l’anniversaire ignoré du putsch raté de 1991 Le vingt-cinquième anniversaire du putsch raté n’a donné lieu à aucune réjouissance en Russie. Aucune commémoration officielle n’a été organisée, ni même un discours prononcé, bien que Vladimir Poutine tire directement son pouvoir de ces jours fiévreux où tout a basculé. Si Boris Eltsine n’était pas monté sur un char pour haranguer la foule et s’opposer aux putschistes devant le parlement de la ­République socialiste soviétique de Russie, l’actuel président russe n’aurait sans doute jamais accédé à la plus haute marche de l’Etat. Un quart de siècle plus tard, c’est en Crimée, la péninsule ukrainienne annexée en 2014, que Vladimir Poutine a choisi de se rendre à la date du 19 août.\n",
      "\n",
      "Résumé concis et structuré (100 mots maximum) :\n",
      "=== GENERATED SUMMARY ===\n",
      "<extra_id_0> à l’intérieur.\n",
      "3\n",
      "=== LABEL SUMMARY ===\n",
      "Le 19 août 1991, huit conservateurs tentent un coup d'État contre Mikhaïl Gorbatchev, retenue en Crimée. Ils s'opposent à un nouvel accord réformateur. Après 5 jours, les putschistes sont arrêtés. Cet échec précipite la chute de l'URSS : dissolution du PCUS, effondrement du régime totalitaire. Le 26 décembre 1991, l'URSS disparaît officiellement. En 2016, Vladimir Poutine, qui tire son pouvoir de cet épisode, célèbre le 25e anniversaire en Crimée, annexée en 2014.\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "assistant_start = \"Résumé concis et structuré (100 mots maximum) :\"\n",
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print('=== PROMPT ===')\n",
    "print(prompt)\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = -1 \n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start+1:])\n",
    "print(len(prediction[response_start+1:].split()))\n",
    "\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=2500, truncation=True)\n",
    "    labels = tokenizer(examples[\"summary\"], padding=\"max_length\", max_length=150, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess datasets\n",
    "dataset_train = dataset_split[\"train\"].map(preprocess_function)\n",
    "dataset_val = dataset_split[\"validation\"].map(preprocess_function)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "dataset_train = dataset_train.remove_columns([\"text\", \"summary\"])\n",
    "dataset_val = dataset_val.remove_columns([\"text\", \"summary\"])\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a data collator for seq2seq tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.697000</td>\n",
       "      <td>2.618326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.987600</td>\n",
       "      <td>2.571890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.523800</td>\n",
       "      <td>2.567580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=3.498820992787679, metrics={'train_runtime': 337.177, 'train_samples_per_second': 4.449, 'train_steps_per_second': 1.112, 'total_flos': 453719162880000.0, 'train_loss': 3.498820992787679, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_training = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(f\"./t5_r{r}\")\n",
    "\n",
    "# Log training information\n",
    "logs = trainer.state.log_history\n",
    "train_losses = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "\n",
    "with open(f\"./t5_r{r}_infos.json\", \"w\") as f:\n",
    "    json.dump({\"train_losses\": train_losses, \"eval_losses\": eval_losses, \"perc_training\":perc_param, \"time_training\":end_training-start_time}, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "Résume précisément le texte suivant en français en 100 mots maximum. Concentre-toi sur les points essentiels sans ajouter d'opinions ni de commentaires. Évite les phrases inutiles et reformule les idées clairement.\n",
      "\n",
      "Texte :\n",
      "Le 24 août 1991, des Moscovites accompagnent les dépouilles des trois victimes du putsch, tuées trois jours plus tôt. Gueorgui Pinkhassov/Magnum C’était encore le temps des vacances, du repos dans les datchas. Le major-colonel du KGB Valeri Chiriaïev dormait dans la sienne, une bicoque sans eau, quelque part dans un trou perdu de la région de Iaroslav, quand la « boîte » fixée au mur s’est mise à grésiller. « On ne savait même plus si ce machin marchait encore », raconte-t-il. L’antique haut-parleur avait été installé dans toutes les demeures soviétiques pour prévenir d’une guerre nucléaire. Le message contenait aussi un code secret pour toutes les forces de sécurité. Et ce matin du 19 août 1991, il y a vingt-cinq ans, Valeri Chiriaïev découvre, stupéfait, qu’un coup d’Etat est en cours à Moscou, mené par les plus conservateurs des dirigeants. L’homme de la glasnost Opposés à la signature, prévue le lendemain, d’un nouvel accord d’association entre les membres d’une URSS réformée et rebaptisée « Union des républiques socialistes souveraines » et non plus « soviétiques », huit hauts dignitaires tentent d’écarter du pouvoir Mikhaïl Gorbatchev, l’homme de la glasnost (la transparence), « retenu » dans sa résidence d’été, à Foros, en Crimée. L’initiative tourne court : en cinq jours, les conspirateurs réunis dans un éphémère Comité pour l’état d’urgence, dont faisait partie Vladimir Krioutchkov, le patron du KGB, sont arrêtés. Le putsch des nostalgiques d’un pouvoir communiste fort a échoué. Ces événements vont précipiter la chute de l’empire. Le Parti communiste de l’Union soviétique (PCUS) est dissous, la statue de ­Félix Dzerjinski, le fondateur de la Tchéka, la police bolchevique à l’origine du KGB, déboulonnée, et quatre mois plus tard, le 26 décembre 1991, à Minsk, la Russie, la Biélorussie et l’Ukraine signent officiellement l’acte de décès de l’URSS. Médusé, le monde assiste à l’effondrement sans violence d’une des grandes puissances de l’après-guerre, et à l’écroulement d’un régime totalitaire. Article réservé à nos abonnés Lire aussi Dans la Russie de Poutine, l’anniversaire ignoré du putsch raté de 1991 Le vingt-cinquième anniversaire du putsch raté n’a donné lieu à aucune réjouissance en Russie. Aucune commémoration officielle n’a été organisée, ni même un discours prononcé, bien que Vladimir Poutine tire directement son pouvoir de ces jours fiévreux où tout a basculé. Si Boris Eltsine n’était pas monté sur un char pour haranguer la foule et s’opposer aux putschistes devant le parlement de la ­République socialiste soviétique de Russie, l’actuel président russe n’aurait sans doute jamais accédé à la plus haute marche de l’Etat. Un quart de siècle plus tard, c’est en Crimée, la péninsule ukrainienne annexée en 2014, que Vladimir Poutine a choisi de se rendre à la date du 19 août.\n",
      "\n",
      "Résumé concis et structuré (100 mots maximum) :\n",
      "=== GENERATED SUMMARY ===\n",
      "<extra_id_0> de l’URSS. Texte : <extra_id_1> le texte en français. Texte en français. Texte en français. L’histoire..  <extra_id_2>...... .. .. . . . . . . . . . . . . . . . . . / ... / / / ' ' / ' ' l  s s r r s n s t s l'  <extra_id_40> l '  <extra_id_41>  <extra_id_41>  <extra_id_24> '  <extra_id_41> T l '  <extra_id_41>  <extra_id_39> <extra_id_39>  <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39>\n",
      "122\n",
      "=== LABEL SUMMARY ===\n",
      "Le 19 août 1991, huit conservateurs tentent un coup d'État contre Mikhaïl Gorbatchev, retenue en Crimée. Ils s'opposent à un nouvel accord réformateur. Après 5 jours, les putschistes sont arrêtés. Cet échec précipite la chute de l'URSS : dissolution du PCUS, effondrement du régime totalitaire. Le 26 décembre 1991, l'URSS disparaît officiellement. En 2016, Vladimir Poutine, qui tire son pouvoir de cet épisode, célèbre le 25e anniversaire en Crimée, annexée en 2014.\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "assistant_start = \"Résumé concis et structuré (100 mots maximum) :\"\n",
    "summary_data = dataset_split['train'][1]['summary']\n",
    "prompt = prepare_prompt(dataset_split['train'][1], summary_included=False)\n",
    "print('=== PROMPT ===')\n",
    "print(prompt)\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "start_index = prediction.find(assistant_start)\n",
    "if start_index != -1:\n",
    "    response_start = start_index + len(assistant_start)\n",
    "else:\n",
    "    response_start = -1 \n",
    "\n",
    "print(\"=== GENERATED SUMMARY ===\")\n",
    "print(prediction[response_start+1:])\n",
    "print(len(prediction[response_start+1:].split()))\n",
    "\n",
    "print(\"=== LABEL SUMMARY ===\")\n",
    "print(summary_data)\n",
    "print(len(summary_data.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(f\"./t5_r{r}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.5700, Recall: 0.4051, F1: 0.4732\n",
      "ROUGEScores - {'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate fine-tuned model\n",
    "rouges_results_finetune, bert_results_finetune = evaluate_model(model, dataset_test, tokenizer, device, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_finetune = {\n",
    "    \"rouge\": rouges_results_finetune,\n",
    "    \"bert\": bert_results_finetune\n",
    "}\n",
    "\n",
    "with open(f\"t5_evaluation_results_finetune_r{r}.json\", \"w\") as f:\n",
    "    json.dump(results_finetune, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.6403, Recall: 0.4503, F1: 0.5285\n",
      "ROUGEScores - {'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate raw model\n",
    "rouges_results_raw, bert_results_raw = evaluate_model(model_raw, dataset_test, tokenizer, device, generation_config)\n",
    "\n",
    "\n",
    "results_raw = {\n",
    "    \"rouge\": rouges_results_raw,\n",
    "    \"bert\": bert_results_raw\n",
    "}\n",
    "\n",
    "with open(\"t5_evaluation_results_raw.json\", \"w\") as f:\n",
    "    json.dump(results_raw, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
